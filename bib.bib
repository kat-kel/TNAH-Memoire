
@online{16LinkingSegmentation,
  title = {16 Linking, Segmentation, and Alignment - The TEI Guidelines},
  url = {https://tei-c.org/release/doc/tei-p5-doc/en/html/SA.html#SASOstdf},
  urldate = {2022-08-25},
  langid = {SCHEME=iso639 en},
  file = {/Users/kellychristensen/Zotero/storage/2FHPL3GU/SA.html}
}

@online{allianceCommunity2020,
  title = {Community},
  author = {Alliance, The ARK},
  date = {2020-11-06T05:16:42+00:00},
  url = {https://arks.org/community/},
  urldate = {2022-08-23},
  abstract = {The ARK Alliance community comprises institutions and people who use or assign ARK identifiers as well as those interested in promoting ARKs and sustaining the open ARK infrastructure, which consis…},
  langid = {american},
  organization = {{ARK Alliance}},
  file = {/Users/kellychristensen/Zotero/storage/KVHGTAIW/community.html}
}

@unpublished{almasDistributedTextServices2021,
  title = {Distributed {{Text Services}} ({{DTS}}): A {{Community-built API}} to {{Publish}} and {{Consume Text Collections}} as {{Linked Data}}},
  shorttitle = {Distributed {{Text Services}} ({{DTS}})},
  author = {Almas, Bridget and Clérice, Thibault and Cayless, Hugh and Jolivet, Vincent and Liuzzo, Pietro Maria and Romanello, Matteo and Robie, Jonathan and Scott, Ian W.},
  date = {2021-03},
  url = {https://hal.archives-ouvertes.fr/hal-03183886},
  urldate = {2022-08-19},
  abstract = {The paper presents the Distributed Text Service API Specification, a community-built effort to facilitate the publication and consumption of texts and their structures as Linked Data. DTS was designed to be as generic as possible, providing simple operations for navigating collections, navigating within a text and retrieving textual content. While the DTS API uses JSON-LD as the serialization format for non-textual data (e.g. descriptive metadata), TEI XML was chosen as the minimum required format for textual data served by the API, in order to guarantee the interoperability of data published by DTS-compliant repositories. The paper describes the DTS API specifications by means of real-world examples, discusses the key design choices that were made, and concludes by providing a list of existing repositories and libraries that support DTS.},
  keywords = {API Specification,FAIR,Interoperability,Text Navigation},
  file = {/Users/kellychristensen/Zotero/storage/5JC8VERD/Almas et al. - 2021 - Distributed Text Services (DTS) a Community-built.pdf}
}

@software{bartzAnnotator2022,
  title = {Annotator},
  author = {Bartz, Alexandre and Janes, Juliette},
  date = {2022-08-01T02:57:34Z},
  origdate = {2021-07-23T13:55:09Z},
  url = {https://github.com/e-ditiones/Annotator},
  urldate = {2022-09-07},
  abstract = {Automatic annotation of XML encoded files.},
  organization = {{E-ditiones}}
}

@inproceedings{bartzExpandingContentModel2021,
  title = {Expanding the Content Model of {{annotationBlock}}},
  booktitle = {Next {{Gen TEI}}, 2021 - {{TEI Conference}} and {{Members}}' {{Meeting}}},
  author = {Bartz, Alexandre and Janes, Juliette and Romary, Laurent and Gambette, Philippe and Bawden, Rachel and Ortiz Suarez, Pedro and Sagot, Benoît and Gabay, Simon},
  date = {2021-10},
  location = {{Virtual, United States}},
  url = {https://hal.archives-ouvertes.fr/hal-03380805},
  urldate = {2022-09-07},
  keywords = {Annotation stand-off,Lemmatisation,Named entity recognition,Reconnaissance d'entité nommée,Stand-off annotations,TEI,TEI encoding model},
  file = {/Users/kellychristensen/Zotero/storage/XZHTPHIY/Bartz et al. - 2021 - Expanding the content model of annotationBlock.pdf}
}

@inproceedings{bawdenAutomaticNormalisationEarly2022,
  title = {Automatic {{Normalisation}} of {{Early Modern French}}},
  author = {Bawden, Rachel and Poinhos, Jonathan and Kogkitsidou, Eleni and Gambette, Philippe and Sagot, Benoît and Gabay, Simon},
  date = {2022-06-20},
  doi = {10.5281/zenodo.5865428},
  url = {https://hal.inria.fr/hal-03540226},
  urldate = {2022-08-11},
  abstract = {Spelling normalisation is a useful step in the study and analysis of historical language texts, whether it is manual analysis by experts or automatic analysis using downstream natural language processing (NLP) tools. Not only does it help to homogenise the variable spelling that often exists in historical texts, but it also facilitates the use of off-the-shelf contemporary NLP tools, if contemporary spelling conventions are used for normalisation. We present FREEMnorm, a new benchmark for the normalisation of Early Modern French (from the 17th century) into contemporary French and provide a thorough comparison of three different normalisation methods: ABA, an alignment-based approach and MT-approaches, (both statistical and neural), including extensive parameter searching, which is often missing in the normalisation literature.},
  eventtitle = {{{LREC}} 2022 - 13th {{Language Resources}} and {{Evaluation Conference}}},
  langid = {english},
  file = {/Users/kellychristensen/Zotero/storage/HP9DRF4T/Bawden et al. - 2022 - Automatic Normalisation of Early Modern French.pdf;/Users/kellychristensen/Zotero/storage/88Z9FP2G/hal-03540226.html}
}

@software{bawdenModFrNormalisation2022,
  title = {{{ModFr-Normalisation}}},
  author = {Bawden, Rachel},
  date = {2022-04-25T15:13:21Z},
  origdate = {2022-01-09T20:40:39Z},
  url = {https://github.com/rbawden/ModFr-Norm},
  urldate = {2022-09-07}
}

@inproceedings{berchmansOpticalCharacterRecognition2014,
  title = {Optical Character Recognition: {{An}} Overview and an Insight},
  shorttitle = {Optical Character Recognition},
  booktitle = {2014 {{International Conference}} on {{Control}}, {{Instrumentation}}, {{Communication}} and {{Computational Technologies}} ({{ICCICCT}})},
  author = {Berchmans, Deepa and Kumar, S S},
  date = {2014-07},
  pages = {1361--1365},
  doi = {10.1109/ICCICCT.2014.6993174},
  abstract = {Many researches are going on in the field of optical character recognition (OCR) for the last few decades and a lot of articles have been published. Also a large number of OCR is available commercially. In this literature a review of the OCR history and the various techniques used for OCR development in the chronological order is being done.},
  eventtitle = {2014 {{International Conference}} on {{Control}}, {{Instrumentation}}, {{Communication}} and {{Computational Technologies}} ({{ICCICCT}})},
  keywords = {Character recognition,document analysis,Feature extraction,glyph recognition,Handwriting recognition,handwritten characters,Hidden Markov models,Image recognition,intelligent character recognition,offline recognition,online recognition,Optical Character Recognition,Optical character recognition software,pattern recognition,printed characters},
  file = {/Users/kellychristensen/Zotero/storage/BCJ4YNBU/6993174.html}
}

@report{bibliothequenationaledefranceRapportActivite20212022,
  title = {Rapport d'activité 2021 de la Bibliothèque nationale de France},
  author = {{Bibliothèque nationale de France}},
  date = {2022-07-01},
  location = {{Paris, France}},
  url = {https://www.bnf.fr/fr/bnf-rapport-dactivite-2021},
  urldate = {2022-08-09},
  abstract = {La Bibliothèque nationale de France (BnF) a publié début juillet son rapport d'activité 2021, dans un contexte marqué par la réouverture du quadrilatère Richelieu prévue en septembre 2022 et après plusieurs confinements liés à la pandémie du Covid.},
  langid = {french},
  file = {/Users/kellychristensen/Zotero/storage/T96I82ZG/70680-rapport-d-activite-2021-de-la-bibliotheque-nationale-de-france.html}
}

@inreference{bobichonCaviarder2011,
  title = {Caviarder},
  booktitle = {Codicologia},
  author = {Bobichon, Philippe},
  date = {2011},
  publisher = {{Institut de recherche et d'histoire des textes}},
  url = {http://codicologia.irht.cnrs.fr/theme/liste_theme/413#tr-868},
  urldate = {2022-07-25}
}

@article{bobichonLexiconMisePage2009,
  title = {Le Lexicon : {{Mise}} En Page et Mise En Texte Des Manuscrits Hébreux, Grecs, Latins, Romans et Arabes},
  shorttitle = {Le Lexicon},
  author = {Bobichon, Philippe},
  date = {2009},
  pages = {81},
  url = {https://cel.archives-ouvertes.fr/cel-00377671},
  urldate = {2022-07-25},
  abstract = {Le lexicon est une publication pédagogique proposant une vue d'ensemble des dispositifs élaborés dans différentes traditions linguistiques et culturelles (domaines latin, roman, grec, hébreu et arabe) pour la mise en page et la « mise en texte » des manuscrits. Ce dossier pédagogique est le fruit des ateliers animés, depuis 2003, dans le cadre du stage d'initiation au manuscrit médiéval proposé chaque automne par l'IRHT.{$<$}br{$>$}Il propose une vue d'ensemble des dispositifs élaborés dans différentes traditions linguistiques et culturelles pour la mise en page et la « mise en texte » des manuscrits.{$<$}br{$><$}br{$>$}Bien avant l'invention de l'imprimerie et des outils modernes d'édition, les scribes de l'Antiquité et du Moyen Âge furent confrontés à une série de problèmes qui sont indépendants de l'époque, du milieu culturel, du champ linguistique et du sens de l'écriture : comment favoriser la consultation des textes ? Comment rendre aisément perceptible leur structure, la hiérarchie des éléments qui les constituent, leurs articulations ? Comment mettre en valeur des passages particulièrement remarquables, maintenir une certaine proximité entre le texte et son/ses commentaires(s), disposer les illustrations, signaler erreurs, interpolations, omissions ou emprunts, introduire diverses corrections ?{$<$}br{$><$}br{$>$}L'acuité de ces questions s'est accentuée à mesure que la lecture, dans un premier temps réservée à une élite et pratiquée le plus souvent à haute voix, devenait une activité silencieuse et plus largement répandue. Les solutions mises en œuvre dans les différentes traditions manuscrites obéissent à des considérations pratiques et esthétiques. Leur confrontation met en évidence certaines spécificités, mais aussi des similitudes qui rendent très vraisemblables, en ce domaine comme ailleurs, emprunts et influences. {$<$}br{$><$}br{$>$}Cette confrontation a semblé d'autant plus riche d'enseignements que les questions relatives à la « mise en scène » des manuscrits, se situent au carrefour de l'analyse codicologique, paléographique, et textuelle. Or, dans les travaux consacrés aux différentes traditions manuscrites, elles n'occupent le plus souvent qu'une place annexe, ne donnant lieu qu'à des remarques sporadiques. De ce point de vue, l'ouvrage publié en 1990 sous la direction de H.-J. Martin et J. Vezin (cf. bibliographie), dont ce dossier s'inspire en grande partie, fait exception.{$<$}br{$><$}br{$>$}L'approche comparative adoptée ici est analogue à celle qui structure l'ouvrage récemment publié par l'IRHT, sous la direction de Paul Géhin : Lire le manuscrit médiéval, Paris, 2005 (cf. bibliographie). Elle correspond plus généralement à l'ensemble des travaux entrepris dans le cadre de l'IRHT, dont les résultats ouvrent de nombreuses pistes de recherche.},
  langid = {english},
  file = {/Users/kellychristensen/Zotero/storage/XSTWB7G2/Bobichon - 2009 - Le lexicon  Mise en page et mise en texte de.pdf;/Users/kellychristensen/Zotero/storage/NH524NHG/cel-00377671.html}
}

@misc{BodelianFirstFolio,
  title = {The {{Bodelian First Folio}}: Digital Facsimile of the {{First Folio}} of {{Shakespeare}}'s Plays},
  shorttitle = {The {{Bodelian First Folio}}},
  publisher = {{Bodleian Arch. G c.7.}},
  url = {http://firstfolio.bodleian.ox.ac.uk/},
  urldate = {2022-08-27}
}

@article{brunetCeQuFaut1987,
  title = {Ce qu’il faut savoir du lecteur optique : l’ancien et le nouveau},
  shorttitle = {Ce qu’il faut savoir du lecteur optique},
  author = {Brunet, Étienne},
  date = {1987},
  journaltitle = {Le médiéviste et l'ordinateur},
  volume = {17},
  number = {1},
  pages = {2--5},
  publisher = {{Persée - Portail des revues scientifiques en SHS}},
  doi = {10.3406/medio.1987.1153},
  url = {https://www.persee.fr/doc/medio_0223-3843_1987_num_17_1_1153},
  urldate = {2022-08-02},
  langid = {fre},
  file = {/Users/kellychristensen/Zotero/storage/G2QCCMQL/medio_0223-3843_1987_num_17_1_1153.html}
}

@article{campsCorpusModelsLemmatisation2021,
  title = {Corpus and {{Models}} for {{Lemmatisation}} and {{POS-tagging}} of {{Classical French Theatre}}},
  author = {Camps, Jean-Baptiste and Gabay, Simon and Fièvre, Paul and Clérice, Thibault and Cafiero, Florian},
  date = {2021-02-14},
  journaltitle = {Journal of Data Mining \& Digital Humanities},
  volume = {2021},
  eprint = {2005.07505},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {6485},
  issn = {2416-5999},
  doi = {10.46298/jdmdh.6485},
  url = {http://arxiv.org/abs/2005.07505},
  urldate = {2022-08-09},
  abstract = {This paper describes the process of building an annotated corpus and training models for classical French literature, with a focus on theatre, and particularly comedies in verse. It was originally developed as a preliminary step to the stylometric analyses presented in Cafiero and Camps [2019]. The use of a recent lemmatiser based on neural networks and a CRF tagger allows to achieve accuracies beyond the current state-of-the art on the in-domain test, and proves to be robust during out-of-domain tests, i.e.up to 20th c.novels.},
  archiveprefix = {arXiv},
  issue = {Digital humanities in...},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/kellychristensen/Zotero/storage/9989KBNC/Camps et al. - 2021 - Corpus and Models for Lemmatisation and POS-taggin.pdf;/Users/kellychristensen/Zotero/storage/8CDAEBBJ/2005.html}
}

@article{campsOuVaPhilologie2018,
  title = {Où va la philologie numérique ?},
  author = {Camps, Jean-Baptiste},
  date = {2018-01},
  journaltitle = {Fabula LHT (Littérature, histoire, théorie)},
  volume = {20},
  url = {https://halshs.archives-ouvertes.fr/halshs-01674953},
  urldate = {2022-08-29},
  abstract = {Si, pour les premiers philologues à l'utiliser dès les années 1950, l'ordinateur était avant tout un outil auquel déléguer des tâches répétitives ou très vastes d'un point de vue quantitatif, je défends dans cet article l'idée que les méthodes computationnelles, ou, pour mieux dire, la philologie tournée vers les données peut être un vecteur puis-sant de transformation pour nos disciplines, au sein du cadre fourni par le quatrième paradigme, émergent, d'une méthode scientifique unifiée. En effet, si la publication électronique d'éditions de texte a souvent, jusqu'à présent, retenu l'attention des cher-cheurs en sciences du texte, le coeur des transformations potentielles réside plutôt dans une approche qui mette les données au centre : en amont, cela signifie tirer profit des méthodes computationnelles pour produire des données dans des quantités ou granula-rités jusque là inenvisageables ; en aval, cette approche est à même de renverser la ma-nière même dont nous formulons nos énoncés ou construisons la connaissance, en pas-sant d'une approche fondée sur des hypothèses préexistantes à un processus dans le-quel on fasse émerger une formalisation à partir des données elles-mêmes, nous éman-cipant ainsi un peu plus de nos postulats les plus ancrés ou des présupposés du sens commun. Tandis que nous enregistrons de nouveaux ensembles de faits sélectionnés dans nos sources, de nouvelles enquêtes ou analyses deviennent possible, pour autant que nous ne restions pas prisonniers des interfaces, limitées par nature et dont la pé-rennité est douteuse. Si « les données sont le résultat important sur le long terme », elles devraient être partagées sans restrictions, tant pour permettre le progrès cumulatif des connaissances, que pour se conformer aux exigences de la recherche scientifique : reproductibilité, réfutabilité. Pour que ce type de science ouverte soit réalisée, nous devrions être moins indulgents envers les revendications abusives de droits patrimo-niaux ou de propriété des textes anciens.},
  langid = {french},
  file = {/Users/kellychristensen/Zotero/storage/KELT7NRZ/Camps - 2018 - Où va la philologie numérique .pdf;/Users/kellychristensen/Zotero/storage/QWPJW5C4/halshs-01674953.html}
}

@article{carlinBnFDataLabService2021,
  title = {Le {{BnF DataLab}}, Un Service Aux Chercheurs En Humanités Numériques},
  author = {Carlin, Marie and Laborderie, Arnaud},
  date = {2021-12},
  journaltitle = {Humanités numériques},
  volume = {4},
  publisher = {{Bruxelles: Humanistica}},
  url = {https://hal-bnf.archives-ouvertes.fr/hal-03285816},
  urldate = {2022-08-11},
  keywords = {Archives de l'Internet,Bibliothèque numérique,Collections numériques,Digital Collections,Digital Humanities,Digital Library,Digitization,French Internet Archives,Humanités numériques,Numérisation},
  file = {/Users/kellychristensen/Zotero/storage/N336RTS2/Carlin and Laborderie - 2021 - Le BnF DataLab, un service aux chercheurs en human.pdf}
}

@report{caronFormatsDonneesPour2021,
  type = {Technical Report},
  title = {Formats de Données Pour La Préservation à Long Terme : La Politique de La {{BnF}}},
  shorttitle = {Formats de Données Pour La Préservation à Long Terme},
  author = {Caron, Bertrand},
  date = {2021-10},
  institution = {{Bibliothèque Nationale de France (Paris)}},
  url = {https://hal-bnf.archives-ouvertes.fr/hal-03374030},
  urldate = {2022-08-23},
  file = {/Users/kellychristensen/Zotero/storage/4DIXRUS7/Caron - 2021 - Formats de données pour la préservation à long ter.pdf}
}

@misc{catcorprojectLetter02633Frederick2021,
  title = {Letter 02633: {{To Frederick II}} (the {{Great}}), 21 {{July}} 1744},
  author = {CatCor Project},
  editor = {Kahn, Andrew and Rubin-Detlev},
  date = {2021},
  publisher = {{CatCor Project, Medieval \& Modern Languages Faculty, University of Oxford}},
  url = {https://catcor.seh.ox.ac.uk/id/letter-02633}
}

@software{chagueHTRUnitedHtrunitedV02022,
  title = {{{HTR-United}}/Htr-United: V0.1.28},
  shorttitle = {{{HTR-United}}/Htr-United},
  author = {Chagué, Alix and Clérice, Thibault and Biay, Sébastien and CVidalG and FloChiff and Vlachou, Matenia and Jacsont, Pauline and Phillip and Constum, Thomas and Hodel, Tobias},
  date = {2022-08-10},
  doi = {10.5281/zenodo.6979746},
  url = {https://zenodo.org/record/6979746},
  urldate = {2022-08-12},
  abstract = {Version 0.1.28 What's Changed Create editer-la-correspondance-de-constance-de-salm-1767-1845.yml by @sbiay in https://github.com/HTR-United/htr-united/pull/77 New Contributors @sbiay made their first contribution in https://github.com/HTR-United/htr-united/pull/77 Full Changelog: https://github.com/HTR-United/htr-united/compare/v0.1.27...v0.1.28},
  organization = {{Zenodo}},
  file = {/Users/kellychristensen/Zotero/storage/FJE6S5UR/6979746.html}
}

@article{chagueHTRUnitedManuMcFrench2022,
  title = {{{HTR-United}} - {{Manu McFrench V1}} ({{Manuscripts}} of {{Modern}} and {{Contemporaneous French}})},
  author = {Chagué, Alix and Clérice, Thibault},
  date = {2022-06-17},
  publisher = {{Zenodo}},
  doi = {10.5281/zenodo.6657809},
  url = {https://zenodo.org/record/6657809},
  urldate = {2022-08-12},
  abstract = {Data were used from all French evenly mixed repository in HTR-United where images and transcription were readily available. Only the data from the 1600 to 2100 were used. 100 pages of Spanish Letters (19th) and a little of English are included (20th). All data follow the same transcription guidelines as far as we know. Superscript is presented with a \^ before the characters. eg. Mme abbreviation is written M\^me in all the dataset we used. Sometime, `{$<>$}` would be used to mark strike-through. All data were produced in eScriptorium with segmentation from Kraken.},
  keywords = {kraken_pytorch},
  file = {/Users/kellychristensen/Zotero/storage/LW9KLGSW/6657809.html}
}

@inproceedings{chagueHTRUnitedMutualisonsVerite2021,
  title = {{{HTR-United}} : {{Mutualisons}} La Vérité de Terrain !},
  shorttitle = {{{HTR-United}}},
  booktitle = {{{DHNord2021}} - {{Publier}}, Partager, Réutiliser Les Données de La Recherche : Les Data Papers et Leurs Enjeux},
  author = {Chagué, Alix and Clérice, Thibault and Romary, Laurent},
  date = {2021-11},
  publisher = {{MESHS}},
  location = {{Lille, France}},
  url = {https://hal.archives-ouvertes.fr/hal-03398740},
  urldate = {2022-08-10},
  keywords = {Contrôle qualité,Datasets,Ground truth data,Handwritten Text Recognition,Jeu de données,Quality Evaluation,Reconnaissance automatique d'écriture manuscrite,Vérité de terrain},
  file = {/Users/kellychristensen/Zotero/storage/FERPIN7F/Chagué et al. - 2021 - HTR-United  Mutualisons la vérité de terrain !.pdf}
}

@software{chagueLECTAUREPContemporaryFrench2022,
  title = {{{LECTAUREP Contemporary French Model}} ({{Administration}})},
  author = {Chagué, Alix},
  date = {2022-05-12},
  url = {https://zenodo.org/record/6542744},
  urldate = {2022-08-12},
  abstract = {Description The model was trained from the ground truth produced by the LECTAUREP Project (Inria \& Archives Nationales) between 2019 and 2022. The training dataset contained many handwriting examples taken from French administrative documents produced between 1742 and 1928. Training and Testing datasets The data was collected from LECTAUREP's ground truth repositories: - lectaurep-bronod v0.0.1 - lectaurep-mariages-et-divorces v.1.0 - lectaurep-repertoires v2.0 12 pages were kept aside to create a test set. The training dataset contained: - 308 files - 19 364 lines - 329 270 characters The test dataset contained: - 12 files - 962 lines - 15 243 characters ~ Transcription standards The transcriptions were created with eScriptorium. They respect what is written (abbreviations are not developed, capitalization follows 19th century practices). Superscripted portions of text are signaled by `\^` and many signatures are transcription with ¥. Training The model was trained using the NFD normalization. Credits The model was trained by Alix Chagué using data created by Aurélia Rostaing, Françoise Limon-Bonnet, Nathalie Denis and Marc Durand. Additional information - more information on the LECTAUREP Project can be found at https://lectaurep.hypotheses.org/ - more information on the model can be found at https://github.com/lectaurep/lectaurep\_base\_model},
  organization = {{Zenodo}},
  keywords = {Contemporary French,French,HTR,kraken_pytorch,recognition model,transcription model},
  file = {/Users/kellychristensen/Zotero/storage/EJTTCQSH/6542744.html}
}

@inproceedings{chagueSharingHTRDatasets2022,
  title = {Sharing {{HTR}} Datasets with Standardized Metadata: The {{HTR-United}} Initiative},
  shorttitle = {Sharing {{HTR}} Datasets with Standardized Metadata},
  author = {Chagué, Alix and Clérice, Thibault},
  date = {2022-06-23},
  url = {https://hal.inria.fr/hal-03703989},
  urldate = {2022-08-12},
  eventtitle = {Documents Anciens et Reconnaissance Automatique Des Écritures Manuscrites},
  langid = {english},
  file = {/Users/kellychristensen/Zotero/storage/WE9LUH4A/Chagué and Clérice - 2022 - Sharing HTR datasets with standardized metadata t.pdf;/Users/kellychristensen/Zotero/storage/2MHWG278/hal-03703989.html}
}

@software{christensenGallicorporaApplication,
  title = {Gallicorpora/Application},
  author = {Christensen, Kelly},
  url = {https://github.com/Gallicorpora/application},
  urldate = {2022-08-21},
  file = {/Users/kellychristensen/Zotero/storage/MXUSAFLW/application.html}
}

@inproceedings{christensenGallicOrporTraitment2022,
  title = {Gallic(Orpor)a: {{Traitment}} Des Sources Textuelles En Diachronie Longue de {{Gallica}}},
  shorttitle = {Gallic(Orpor)a},
  booktitle = {{{DataLab}} de La {{BnF}}},
  author = {Christensen, Kelly and Pinche, Ariane and Gabay, Simon},
  date = {2022-06},
  location = {{Paris, France}},
  url = {https://hal.archives-ouvertes.fr/hal-03716534},
  urldate = {2022-08-09},
  file = {/Users/kellychristensen/Zotero/storage/4NVVI9G5/Christensen et al. - 2022 - Gallic(orpor)a Traitment des sources textuelles e.pdf}
}

@software{clericeDeucalionModeleAncien2019,
  title = {Deucalion, {{Modèle Ancien Francais}} (0.2.0)},
  author = {Clérice, Thibault and Camps, Jean-Baptiste and Pinche, Ariane},
  date = {2019-06-03},
  doi = {10.5281/zenodo.3237455},
  url = {https://zenodo.org/record/3237455},
  urldate = {2022-08-09},
  abstract = {No description provided.},
  organization = {{Zenodo}},
  file = {/Users/kellychristensen/Zotero/storage/KPLKJ8XU/3237455.html}
}

@software{clericeHTRVXHTRValidation2021,
  title = {{{HTRVX}}, {{HTR Validation}} with {{XSD}}},
  author = {Clérice, Thibault and Pinche, Ariane},
  date = {2021-09},
  doi = {10.5281/zenodo.5359963},
  url = {https://github.com/HTR-United/HTRVX},
  urldate = {2022-08-12},
  abstract = {HTRVX : HTR Validation with XSD},
  version = {0.0.1}
}

@software{clericePieExtendedExtension2020a,
  title = {Pie {{Extended}}, an Extension for {{Pie}} with Pre-Processing and Post-Processing},
  shorttitle = {Pie {{Extended}}},
  author = {Clérice, Thibault},
  date = {2020-06},
  doi = {10.5281/zenodo.6534764},
  url = {https://zenodo.org/record/6534764},
  urldate = {2022-09-07},
  organization = {{Zenodo}},
  file = {/Users/kellychristensen/Zotero/storage/TDYCYTX8/6534764.html}
}

@dataset{clericeYALTAiSegmontoManuscript2022,
  title = {{{YALTAi}}: {{Segmonto Manuscript}} and {{Early Printed Book Dataset}}},
  shorttitle = {{{YALTAi}}},
  author = {Clérice, Thibault},
  date = {2022-07-10},
  publisher = {{Zenodo}},
  doi = {10.5281/zenodo.6814770},
  url = {https://zenodo.org/record/6814770},
  urldate = {2022-08-12},
  abstract = {This dataset has been built to train a segmentation model. It contains ALTO and YOLOv5 formats This dataset is derived from: CREMMA Medieval ( Pinche, A. (2022). Cremma Medieval (Version Bicerin 1.1.0) [Data set]. https://github.com/HTR-United/cremma-medieval ) CREMMA Medieval Lat (Clérice, T. and~Vlachou-Efstathiou, M. (2022). Cremma Medieval Latin [Data set]. https://github.com/HTR-United/cremma-medieval-lat ) Eutyches. (Vlachou-Efstathiou, M. Voss.Lat.O.41 - Eutyches "de uerbo" glossed [Data set]. https://github.com/malamatenia/Eutyches) Gallicorpora HTR-Incunable-15e-Siecle ( Pinche, A., Gabay, S., Leroy, N., \& Christensen, K. Données HTR incunable du 15e siècle [Computer software]. https://github.com/Gallicorpora/HTR-incunable-15e-siecle ) Gallicorpora HTR-MSS-15e-Siecle ( Pinche, A., Gabay, S., Leroy, N., \& Christensen, K. Données HTR manuscrits du 15e siècle [Computer software]. https://github.com/Gallicorpora/HTR-MSS-15e-Siecle ) Gallicorpora HTR-imprime-gothique-16e-siecle ( Pinche, A., Gabay, S., Vlachou-Efstathiou, M., \& Christensen, K. HTR-imprime-gothique-16e-siecle [Computer software]. https://github.com/Gallicorpora/HTR-imprime-gothique-16e-siecle ) + a few hundred newly annotated data, specifically the test set which is completely novel and based on early prints and manuscripts. ~ Dataset Number of images Train 854 Dev 154 Test 139},
  file = {/Users/kellychristensen/Zotero/storage/X264W5J6/6814770.html}
}

@inproceedings{coquenetHandwrittenTextRecognition2021,
  title = {Handwritten Text Recognition: From Isolated Text Lines to Whole Documents},
  shorttitle = {Handwritten Text Recognition},
  booktitle = {{{ORASIS}} 2021},
  author = {Coquenet, Denis and Chatelain, Clément and Paquet, Thierry},
  date = {2021-09},
  publisher = {{Centre National de la Recherche Scientifique [CNRS]}},
  location = {{Saint Ferréol, France}},
  url = {https://hal.archives-ouvertes.fr/hal-03339648},
  urldate = {2022-08-04},
  abstract = {The handwriting recognition task is largely dominated by deep neural networks. However, it remains challenging for these advanced computer vision systems. Recently, the models have become more sophisticated, moving from line-level recognition to paragraph-level and even page-level recognition. In this paper, we will study those advances and the constraints that come with them, mainly focusing on two models we proposed: the Simple Predict \& Align Network and the Vertical Attention Network. Both handle paragraph images, and we outperformed the state of the art on three datasets: RIMES, IAM and READ 2016},
  keywords = {Attention,FCN,Handwritten text recognition,Seq2seq model},
  file = {/Users/kellychristensen/Zotero/storage/XV3NP84W/hal-03339648.html}
}

@article{daherEtudeDynamiqueEcritures2011,
  title = {Étude de la dynamique des écritures médiévales : analyse et classification des formes écrites},
  shorttitle = {Étude de la dynamique des écritures médiévales},
  author = {Daher, Hani and Eglin, Véronique and Bres, Stéphane and Vincent, Nicole},
  date = {2011},
  journaltitle = {Gazette du livre médiéval},
  volume = {56},
  number = {1},
  pages = {21--41},
  publisher = {{Persée - Portail des revues scientifiques en SHS}},
  doi = {10.3406/galim.2011.1980},
  url = {https://www.persee.fr/doc/galim_0753-5015_2011_num_56_1_1980},
  urldate = {2022-08-01},
  langid = {fre},
  file = {/Users/kellychristensen/Zotero/storage/PAXA67M9/Daher et al. - 2011 - Étude de la dynamique des écritures médiévales  a.pdf;/Users/kellychristensen/Zotero/storage/7ABKGUV9/galim_0753-5015_2011_num_56_1_1980.html}
}

@software{DigiPalDigitalResource2011,
  title = {{{DigiPal}} : {{Digital Resource}} and {{Database}} of {{Palaeography}}, {{Manuscripts}} and {{Diplomatic}}.},
  shorttitle = {{{DigiPal}}},
  date = {2011-22},
  location = {{London}},
  url = {http://www.digipal.eu/}
}

@inproceedings{gabayAutomatingArtlExtracting2021,
  title = {Automating {{Artl}}@s – Extracting Data from Exhibition Catalogues},
  booktitle = {{{EADH}} 2021 - {{Second International Conference}} of the {{European Association}} for {{Digital Humanities}}},
  author = {Gabay, Simon and Topalov, Barbara and Corbières, Caroline and Rondeau Du Noyer, Lucie and Joyeux-Prunel, Béatrice and Romary, Laurent},
  date = {2021-09},
  location = {{Krasnoyarsk, Russia}},
  url = {https://hal.archives-ouvertes.fr/hal-03331838},
  urldate = {2022-08-23},
  abstract = {Catalogues, which have been published for centuries, are an extremely precious resource for scholars. Using the Artl@s database as an example, where exhibition catalogues are transformed into a georeferenced database, we question the possibility of an (almost) automatic transformation of pdfs into semantically annotated data. To do so, we present and analyse the graphic organisation of exhibition catalogues, before exploring a possible modeling into TEI (involving possible enhancement of the guidelines).},
  keywords = {Art history,Catalogues,Extraction d'information,Histoire de l'art,Information extraction,Text encoding initiative},
  file = {/Users/kellychristensen/Zotero/storage/FI344WNB/Gabay et al. - 2021 - Automating Artl@s – extracting data from exhibitio.pdf}
}

@misc{gabayEditiones17thFrench2018,
  title = {E-Ditiones, 17th c. {{French}} Sources},
  author = {Gabay, Simon},
  date = {2018-11},
  publisher = {{DARIAH}},
  url = {https://hal.archives-ouvertes.fr/hal-02388415},
  urldate = {2022-08-10},
  keywords = {Bibliothèque numérique,Digital library,French literature – 17th century,Littérature française – 17e siècle},
  annotation = {Published: Workshop DARIAH-CH},
  file = {/Users/kellychristensen/Zotero/storage/7LX8GK56/Gabay - 2018 - E-ditiones, 17th c. French sources.pdf}
}

@inproceedings{gabayFreEMAlemBERT2022,
  title = {From {{FreEM}} to {{D}}'{{AlemBERT}}},
  booktitle = {Proceedings of the 13th {{Language Resources}} and {{Evaluation Conference}}},
  author = {Gabay, Simon and Ortiz Suarez, Pedro and Bartz, Alexandre and Chagué, Alix and Bawden, Rachel and Gambette, Philippe and Sagot, Benoît},
  date = {2022-06},
  publisher = {{European Language Resources Association}},
  location = {{Marseille, France}},
  url = {https://hal.inria.fr/hal-03596653},
  urldate = {2022-08-10},
  abstract = {Language models for historical states of language are becoming increasingly important to allow the optimal digitisation and analysis of old textual sources. Because these historical states are at the same time more complex to process and more scarce in the corpora available, specific efforts are necessary to train natural language processing (NLP) tools adapted to the data. In this paper, we present our efforts to develop NLP tools for Early Modern French (historical French from the 16th to the 18th centuries). We present the FreEMmax corpus of Early Modern French and D'AlemBERT, a RoBERTa-based language model trained on FreEMmax. We evaluate the usefulness of D'AlemBERT by fine-tuning it on a part-of-speech tagging task, outperforming previous work on the test set. Importantly, we find evidence for the transfer learning capacity of the language model, since its performance on lesser-resourced time periods appears to have been boosted by the more resourced ones. We release D'AlemBERT and the open-sourced subpart of the FreEMmax corpus.},
  keywords = {Corpus creation,Création de corpus,Digital humanities,Early Modern French,Français classique,Humanités Numériques,Language modelling,Langues peu dotées,Less-resourced languages,Modèle de langue neuronal,Modélisation linguistique,Neural language representation models,Partie du discours,POS tagging},
  file = {/Users/kellychristensen/Zotero/storage/HKFU8AXU/hal-03596653v1.html}
}

@software{gabayFreEMcorporaFreEMnormFreEM2022,
  title = {{{FreEM-corpora}}/{{FreEMnorm}}: {{FreEM}} Norm {{Parallel}} Corpus},
  shorttitle = {{{FreEM-corpora}}/{{FreEMnorm}}},
  author = {Gabay, Simon},
  date = {2022-01-17},
  doi = {10.5281/zenodo.5865428},
  url = {https://zenodo.org/record/5865428},
  urldate = {2022-08-11},
  abstract = {Parallel corpus for Early Modern French},
  organization = {{Zenodo}},
  file = {/Users/kellychristensen/Zotero/storage/Z9TPEL6Y/5865428.html}
}

@misc{gabayOCR17GroundTruth2020,
  title = {{{OCR17}}: {{Ground Truth}} and {{Models}} for 17th c. {{French Prints}} (and Hopefully More)},
  shorttitle = {{{OCR17}}},
  author = {Gabay, Simon and Clérice, Thibault and Reul, Christian},
  date = {2020-05},
  url = {https://hal.archives-ouvertes.fr/hal-02577236},
  urldate = {2022-08-12},
  keywords = {17th c French,Construction de corpus,Corpus building,Data paper,Donnees,OCR,Training data,XVIIème siècle},
  file = {/Users/kellychristensen/Zotero/storage/5JTIN8E2/Gabay et al. - 2020 - OCR17 Ground Truth and Models for 17th c. French .pdf}
}

@inproceedings{gabayProjetFREEMRessources2022,
  title = {Le Projet {{FREEM}} : Ressources, Outils et Enjeux Pour l'étude Du Français d'{{Ancien Régime}}},
  shorttitle = {Le Projet {{FREEM}}},
  booktitle = {{{TALN}} 2022 - {{Traitement Automatique}} Des {{Langues Naturelles}}},
  author = {Gabay, Simon and Ortiz Suarez, Pedro and Bawden, Rachel and Bartz, Alexandre and Gambette, Philippe and Sagot, Benoît},
  editor = {Estève, Yannick and Jiménez, Tania and Parcollet, Titouan and Zanon Boito, Marcely},
  date = {2022-06},
  pages = {154--165},
  publisher = {{ATALA}},
  location = {{Avignon, France}},
  url = {https://hal.archives-ouvertes.fr/hal-03701524},
  urldate = {2022-08-10},
  keywords = {Diachronic linguistics,linguistic normalisation,Linguistique diachronique,Named-entity recognition,Normalisation,Reconnaissance d'entités nommées},
  file = {/Users/kellychristensen/Zotero/storage/HR4YIFPV/Gabay et al. - 2022 - Le projet FREEM  ressources, outils et enjeux pou.pdf}
}

@inproceedings{gabaySegmOnto2021,
  title = {{{SegmOnto}}},
  booktitle = {Création de Modèle(s) {{HTR}} Pour Les Documents Médiévaux En Ancien Français et Moyen Français Entre Le {{Xe-XIVe}} Siècle},
  author = {Gabay, Simon and Camps, Jean-Baptiste and Pinche, Ariane},
  date = {2021-11},
  publisher = {{Ecole nationale des chartes | PSL}},
  location = {{Paris, France}},
  url = {https://hal.archives-ouvertes.fr/hal-03481089},
  urldate = {2022-07-25},
  keywords = {Controlled vocabularies,Handwritten Text Recognition,Manuscript,Manuscrits,Reconnaissance automatique d'écriture manuscrite,Vocabulaire contrôlé},
  file = {/Users/kellychristensen/Zotero/storage/EDZ8FJM7/hal-03481089.html}
}

@inproceedings{gabaySegmOntoCommonVocabulary2021,
  title = {{{SegmOnto}}: Common Vocabulary and Practices for Analysing the Layout of Manuscripts (and More)},
  shorttitle = {{{SegmOnto}}},
  booktitle = {1st {{International Workshop}} on {{Computational Paleography}} ({{IWCP}}@{{ICDAR}} 2021)},
  author = {Gabay, Simon and Camps, Jean-Baptiste and Pinche, Ariane and Jahan, Claire},
  date = {2021-09},
  location = {{Lausanne, Switzerland}},
  url = {https://hal.archives-ouvertes.fr/hal-03336528},
  urldate = {2022-08-09},
  keywords = {Analyse de disposition,controlled vocabulary,Imprimés modernes,layout analysis,manuscripts,Manuscrits,old prints,Vocabulaire controlé},
  file = {/Users/kellychristensen/Zotero/storage/5SL2XWCM/Gabay et al. - 2021 - SegmOnto common vocabulary and practices for anal.pdf}
}

@inproceedings{gabayStandardizingLinguisticData2020,
  title = {Standardizing Linguistic Data: Method and Tools for Annotating (Pre-Orthographic) {{French}}},
  shorttitle = {Standardizing Linguistic Data},
  booktitle = {Proceedings of the 2nd {{International Digital Tools}} \& {{Uses Congress}} ({{DTUC}} '20)},
  author = {Gabay, Simon and Clérice, Thibault and Camps, Jean-Baptiste and Tanguy, Jean-Baptiste and Gille-Levenson, Matthias},
  date = {2020-10},
  location = {{Hammamet, Tunisia}},
  doi = {10.1145/3423603.3423996},
  url = {https://hal.archives-ouvertes.fr/hal-03018381},
  urldate = {2022-08-12},
  abstract = {With the development of big corpora of various periods, it becomes crucial to standardise linguistic annotation (e.g. lemmas, POS tags, morphological annotation) to increase the interoperability of the data produced, despite diachronic variations. In the present paper, we describe both methodologically (by proposing annotation principles) and technically (by creating the required training data and the relevant models) the production of a linguistic tagger for (early) modern French (16-18th c.), taking as much as possible into account already existing standards for contemporary and, especially, medieval French.},
  keywords = {Etiquetage morpho-syntaxique,lemmatisation,Lemmatisation,linguistic annotation,POS-tagging,POStagging,pre-orthographic language},
  file = {/Users/kellychristensen/Zotero/storage/2ZQ6IGIU/Gabay et al. - 2020 - Standardizing linguistic data method and tools fo.pdf}
}

@thesis{gabriacDeepNaturalLanguage2021,
  type = {phdthesis},
  title = {Deep {{Natural Language Processing}} for {{User Representation}}},
  author = {de Gabriac, Clara Gainon de Forsan},
  date = {2021-12-13},
  institution = {{Sorbonne Université}},
  url = {https://tel.archives-ouvertes.fr/tel-03545621},
  urldate = {2022-09-07},
  abstract = {The last decade has witnessed the impressive expansion of Deep Learning (DL) methods, both in academic research and the private sector. This success can be explained by the ability DL to model ever more complex entities. In particular, Representation Learning methods focus on building latent representations from heterogeneous data that are versatile and re-usable, namely in Natural Language Processing (NLP). In parallel, the ever-growing number of systems relying on user data brings its own lot of challenges. This work proposes methods to leverage the representation power of NLP in order to learn rich and versatile user representations.Firstly, we detail the works and domains associated with this thesis. We study Recommendation. We then go over recent NLP advances and how they can be applied to leverage user-generated texts, before detailing Generative models.Secondly, we present a Recommender System (RS) that is based on the combination of a traditional Matrix Factorization (MF) representation method and a sentiment analysis model. The association of those modules forms a dual model that is trained on user reviews for rating prediction. Experiments show that, on top of improving performances, the model allows us to better understand what the user is really interested in in a given item, as well as to provide explanations to the suggestions made.Finally, we introduce a new task-centered on UR: Professional Profile Learning. We thus propose an NLP-based framework, to learn and evaluate professional profiles on different tasks, including next job generation.},
  langid = {english},
  file = {/Users/kellychristensen/Zotero/storage/9PY3B6LD/Gabriac - 2021 - Deep Natural Language Processing for User Represen.pdf;/Users/kellychristensen/Zotero/storage/T76HUHAA/tel-03545621.html}
}

@book{gacekArabicManuscriptTradition2001,
  title = {The Arabic Manuscript Tradition : A Glossary of Technical Terms and Bibliography},
  author = {Gacek, Adam},
  date = {2001},
  series = {Handbook of {{Oriental Studies}}},
  number = {1, The Near and Middle East},
  publisher = {{Brill}},
  location = {{Leiden}},
  isbn = {ISBN 90-04-12061-0},
  pagetotal = {269}
}

@article{GlossaireCodicologiqueFrancaisarabe2002,
  title = {Glossaire codicologique français-arabe},
  date = {2002},
  journaltitle = {Gazette du livre médiéval},
  volume = {40},
  number = {1},
  pages = {79--80},
  publisher = {{Persée - Portail des revues scientifiques en SHS}},
  url = {https://www.persee.fr/doc/galim_0753-5015_2002_num_40_1_1563},
  urldate = {2022-07-25},
  langid = {fre},
  file = {/Users/kellychristensen/Zotero/storage/YT5ASFJM/galim_0753-5015_2002_num_40_1_1563.html}
}

@article{goodrichKurzweilReadingMachine1979,
  title = {Kurzweil {{Reading Machine}}: {{A Partial Evaluation}} of {{Its Optical Character Recognition Error Rate}}},
  shorttitle = {Kurzweil {{Reading Machine}}},
  author = {Goodrich, Gregory},
  date = {1979-01-12},
  journaltitle = {Journal of Visual Impairment and Blindness},
  shortjournal = {Journal of Visual Impairment and Blindness},
  abstract = {A study designed to assess the ability of the Kurzweil reading machine (a speech reading device for the visually handicapped) to read three different type styles produced by five different means indicated that the machines tested had different error rates depending upon the means of producing the copy and upon the type style used. (Author/CL)},
  file = {/Users/kellychristensen/Zotero/storage/JATPXPSA/Goodrich - 1979 - Kurzweil Reading Machine A Partial Evaluation of .pdf}
}

@article{govindanCharacterRecognitionReview1990,
  title = {Character Recognition — {{A}} Review},
  author = {Govindan, V. K. and Shivaprasad, A. P.},
  date = {1990},
  journaltitle = {Pattern Recognition},
  volume = {23},
  number = {7},
  pages = {671},
  issn = {0031-3203},
  url = {https://www.academia.edu/6986960/Character_recognition_A_review},
  urldate = {2022-08-05},
  abstract = {Character recognition - A review},
  langid = {english},
  file = {/Users/kellychristensen/Zotero/storage/ACCVNSEK/Govindan and Shivaprasad - 1990 - Character recognition — A review.pdf}
}

@inproceedings{guibonParsingPoorlyStandardized2014,
  title = {Parsing {{Poorly Standardized Language Dependency}} on {{Old French}}},
  booktitle = {Thirteenth {{International Workshop}} on {{Treebanks}} and {{Linguistic Theories}} ({{TLT13}})},
  author = {Guibon, Gaël and Tellier, Isabelle and Constant, Mathieu and Prévost, Sophie and Gerdes, Kim},
  editor = {Henrich, V. and Hinrichs, E. and de Kok, D. and Przepiórkowski, P. Osenova \& A.},
  date = {2014-12},
  series = {Proceedings of the {{Thirteenth International Workshop}} on {{Treebanks}} and {{Linguistic Theories}} ({{TLT13}})},
  pages = {51--61},
  location = {{Tübingen, Germany}},
  url = {https://hal.archives-ouvertes.fr/hal-01250959},
  urldate = {2022-08-10},
  abstract = {This paper presents results of dependency parsing of Old French, a language which is poorly standardized at the lexical level, and which displays a relatively free word order. The work is carried out on five distinct sample texts extracted from the dependency treebank Syntactic Reference Corpus of Medieval French (SRCMF). Following Achim Stein's previous work, we have trained the Mate parser on each sub-corpus and cross-validated the results. We show that the parsing efficiency is diminished by the greater lexical variation of Old French compared to parse results on modern French. In order to improve the result of the POS tagging step in the parsing process, we applied a pre-treatment to the data, comparing two distinct strategies: one using a slightly post-treated version of the TreeTagger trained on Old French by Stein, and a CRF trained on the texts, enriched with external resources. The CRF version outperforms every other approach.},
  keywords = {corpus exploration,dependency parsing,machine learning,Old French,POS labelling},
  file = {/Users/kellychristensen/Zotero/storage/PMSX9XAH/Guibon et al. - 2014 - Parsing Poorly Standardized Language Dependency on.pdf}
}

@online{HTRUnited,
  title = {{{HTR United}}},
  url = {https://github.com/HTR-United},
  urldate = {2022-08-12},
  file = {/Users/kellychristensen/Zotero/storage/RHB584K7/HTR-United.html}
}

@article{impedovoMoreTwentyYears2014,
  title = {More than Twenty Years of Advancements on {{Frontiers}} in Handwriting Recognition},
  author = {Impedovo, Sebastiano},
  date = {2014-03-01},
  journaltitle = {Pattern Recognition},
  shortjournal = {Pattern Recognition},
  series = {Handwriting {{Recognition}} and Other {{PR Applications}}},
  volume = {47},
  number = {3},
  pages = {916--928},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2013.05.027},
  url = {https://www.sciencedirect.com/science/article/pii/S0031320313002513},
  urldate = {2022-08-29},
  abstract = {In this paper the results of the research in handwriting/handwritten character recognition in about the last quarter of a century are reported, illustrating the results presented during the International Workshop on Frontiers in Handwriting Recognition (IWFHR) and the International Conference on Frontiers in Handwriting Recognition (ICFHR).},
  langid = {english},
  keywords = {Handwriting recognition,Pattern recognition,Systems for PR},
  file = {/Users/kellychristensen/Zotero/storage/XDWU4YTG/Impedovo - 2014 - More than twenty years of advancements on Frontier.pdf;/Users/kellychristensen/Zotero/storage/TH8RI4VD/S0031320313002513.html}
}

@inproceedings{janesAutomaticTEIEncoding2021,
  title = {Towards Automatic {{TEI}} Encoding via Layout Analysis},
  booktitle = {Fantastic Future 21, 3rd {{International Conference}} on {{Artificial Intelligence}} for {{Librairies}}, {{Archives}} and {{Museums}}},
  author = {Janes, Juliette and Pinche, Ariane and Jahan, Claire and Gabay, Simon},
  date = {2021-12},
  publisher = {{AI for Libraries, Archives, and Museums (ai4lam)}},
  location = {{Paris, France}},
  url = {https://hal.archives-ouvertes.fr/hal-03527287},
  urldate = {2022-08-09},
  file = {/Users/kellychristensen/Zotero/storage/A5NMDGWE/Janes et al. - 2021 - Towards automatic TEI encoding via layout analysis.pdf}
}

@incollection{jentschTextDataDigitization2021,
  title = {From {{Text}} to {{Data Digitization}}, {{Text Analysis}} and {{Corpus Linguistics}}},
  shorttitle = {From {{Text}} to {{Data Digitization}}, {{Text Analysis}} and {{Corpus Linguistics}}},
  booktitle = {Digital {{Methods}} in the {{Humanities}}: {{Challenges}}, {{Ideas}}, {{Perspectives}}},
  author = {Jentsch, Patrick and Porada, Stephan},
  editor = {Schwandt, Silke},
  date = {2021},
  publisher = {{Bielefeld University Press}},
  file = {/Users/kellychristensen/Zotero/storage/XYAIL59F/j.ctv2f9xskk.6.pdf}
}

@software{kiesslingKrakenOCRSystem2022,
  title = {The {{Kraken OCR}} System},
  author = {Kiessling, Benjamin},
  date = {2022-04},
  origdate = {2015-05-19T09:24:38Z},
  url = {https://kraken.re},
  urldate = {2022-08-02},
  abstract = {OCR engine for all the languages},
  version = {4.1.2}
}

@inproceedings{kiesslingKrakenUniversalText2019,
  title = {Kraken - an {{Universal Text Recognizer}} for the {{Humanities}}},
  author = {Kiessling, Benjamin},
  date = {2019},
  url = {https://dh-abstracts.library.cmu.edu/works/9912},
  urldate = {2022-08-10},
  eventtitle = {{{ADHO}} 2019 - {{Utrecht}}},
  langid = {english},
  file = {/Users/kellychristensen/Zotero/storage/RKJM3C3D/9912.html}
}

@article{lassnerPublishingOCRGround2021,
  title = {Publishing an {{OCR}} Ground Truth Data Set for Reuse in an Unclear Copyright Setting. {{Two}} Case Studies with Legal and Technical Solutions to Enable a Collective {{OCR}} Ground Truth Data Set Effort},
  author = {Lassner, David and Coburger, Julius and Neudecker, Clemens and Baillot, Anne},
  date = {2021},
  journaltitle = {Fabrikation von Erkenntnis – Experimente in den Digital Humanities. Hg. von Manuel Burghardt},
  volume = {Lisa Dieckmann},
  pages = {5)},
  publisher = {{Zeitschrift für digitale Geisteswissenschaften - ZfdG}},
  doi = {10.17175/SB005_006},
  url = {https://zfdg.de/sb005_006},
  urldate = {2022-08-10},
  abstract = {We present an OCR ground truth data set for historical prints and show improvement of recognition results over baselines with training on this data. We reflect on reusability of the ground truth data set based on two experiments that look into the legal basis for reuse of digitized document images in the case of 19th century English and German books. We propose a framework for publishing ground truth data even when digitized document images cannot be easily redistributed.},
  editora = {Herzog August Bibliothek},
  editoratype = {collaborator},
  langid = {english},
  version = {1.0},
  file = {/Users/kellychristensen/Zotero/storage/SGUYBJGG/Lassner et al. - 2021 - Publishing an OCR ground truth data set for reuse .pdf}
}

@online{libraryofcongressALTOTechnicalMetadata,
  title = {{{ALTO}}: {{Technical Metadata}} for {{Layout}} and {{Text Objects}}},
  author = {Library of Congress},
  url = {https://www.loc.gov/standards/alto/},
  urldate = {2022-08-22},
  file = {/Users/kellychristensen/Zotero/storage/EPXQA6DX/alto.html}
}

@book{maniaciTerminologiaLibroManoscritto1996,
  title = {Terminologia Des Libro Manoscritto},
  author = {Maniaci, Marilena},
  date = {1996},
  series = {Addenda},
  number = {3},
  publisher = {{Istituto centrale per la patologia del libro}},
  location = {{Rome}}
}

@online{ManuelUNIMARCFormat,
  title = {Manuel UNIMARC : format bibliographique},
  shorttitle = {Manuel UNIMARC},
  url = {https://www.transition-bibliographique.fr/unimarc/manuel-unimarc-format-bibliographique/},
  urldate = {2022-08-27},
  abstract = {La description, zone par zone, du format bibliographique est maintenue par l’Ifla et traduite dès que possible par le CfU.},
  langid = {french},
  organization = {{Transition bibliographique - Programme national}},
  file = {/Users/kellychristensen/Zotero/storage/TE9L8YKT/manuel-unimarc-format-bibliographique.html}
}

@inproceedings{martinCamemBERTTastyFrench2020,
  title = {{{CamemBERT}}: A {{Tasty French Language Model}}},
  shorttitle = {{{CamemBERT}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Martin, Louis and Muller, Benjamin and Ortiz Suárez, Pedro Javier and Dupont, Yoann and Romary, Laurent and de la Clergerie, Éric and Seddah, Djamé and Sagot, Benoît},
  options = {useprefix=true},
  date = {2020-07},
  pages = {7203--7219},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.acl-main.645},
  url = {https://aclanthology.org/2020.acl-main.645},
  urldate = {2022-08-11},
  abstract = {Pretrained language models are now ubiquitous in Natural Language Processing. Despite their success, most available models have either been trained on English data or on the concatenation of data in multiple languages. This makes practical use of such models –in all languages except English– very limited. In this paper, we investigate the feasibility of training monolingual Transformer-based language models for other languages, taking French as an example and evaluating our language models on part-of-speech tagging, dependency parsing, named entity recognition and natural language inference tasks. We show that the use of web crawled data is preferable to the use of Wikipedia data. More surprisingly, we show that a relatively small web crawled dataset (4GB) leads to results that are as good as those obtained using larger datasets (130+GB). Our best performing model CamemBERT reaches or improves the state of the art in all four downstream tasks.},
  eventtitle = {{{ACL}} 2020},
  file = {/Users/kellychristensen/Zotero/storage/KBTVEMGY/Martin et al. - 2020 - CamemBERT a Tasty French Language Model.pdf}
}

@dataset{meierEeditionesShakespeareVersion2020,
  title = {Eeditiones/Shakespeare: {{Version}} for 6.0 {{RC}} of {{TEI Publisher}}},
  shorttitle = {Eeditiones/Shakespeare},
  author = {Meier, Wolfgang and {anton}},
  date = {2020-06-09},
  publisher = {{Zenodo}},
  doi = {10.5281/zenodo.3886900},
  url = {https://zenodo.org/record/3886900},
  urldate = {2022-08-27},
  abstract = {A demo generated from TEI Publisher to browse the Bodleian First Folio of Shakespeare Plays},
  keywords = {Demo,TEI,TEI Publisher},
  file = {/Users/kellychristensen/Zotero/storage/Q5AY9F7Q/3886900.html}
}

@online{METSMetadataEncoding,
  title = {METS : Metadata Encoding and Transmission Standard},
  shorttitle = {METS},
  url = {https://www.bnf.fr/fr/mets-metadata-encoding-and-transmission-standard},
  urldate = {2022-08-23},
  abstract = {BnF - Site institutionnel},
  langid = {french},
  organization = {{BnF - Site institutionnel}},
  file = {/Users/kellychristensen/Zotero/storage/WFGWMZY5/mets-metadata-encoding-and-transmission-standard.html}
}

@book{misaGenderCodesWhy2011,
  title = {Gender {{Codes}}: {{Why Women Are Leaving Computing}}},
  shorttitle = {Gender {{Codes}}},
  author = {Misa, Thomas J.},
  date = {2011-09-14},
  eprint = {EjDYh_KHls8C},
  eprinttype = {googlebooks},
  publisher = {{John Wiley \& Sons}},
  abstract = {The computing profession faces a serious gender crisis. Today, fewer women enter computing than anytime in the past 25 years. This book provides an unprecedented look at the history of women and men in computing, detailing how the computing profession emerged and matured, and how the field became male coded. Women's experiences working in offices, education, libraries, programming, and government are examined for clues on how and where women succeeded—and where they struggled. It also provides a unique international dimension with studies examining the U.S., Great Britain, Germany, Norway, and Greece. Scholars in history, gender/women's studies, and science and technology studies, as well as department chairs and hiring directors will find this volume illuminating.},
  isbn = {978-1-118-03513-9},
  langid = {english},
  pagetotal = {326},
  keywords = {Computers / Computer Science,Computers / General}
}

@article{muehlbergerTransformingScholarshipArchives2019,
  title = {Transforming Scholarship in the Archives through Handwritten Text Recognition: {{Transkribus}} as a Case Study},
  shorttitle = {Transforming Scholarship in the Archives through Handwritten Text Recognition},
  author = {Muehlberger, Guenter and Seaward, Louise and Terras, Melissa and Ares Oliveira, Sofia and Bosch, Vicente and Bryan, Maximilian and Colutto, Sebastian and Déjean, Hervé and Diem, Markus and Fiel, Stefan and Gatos, Basilis and Greinoecker, Albert and Grüning, Tobias and Hackl, Guenter and Haukkovaara, Vili and Heyer, Gerhard and Hirvonen, Lauri and Hodel, Tobias and Jokinen, Matti and Kahle, Philip and Kallio, Mario and Kaplan, Frederic and Kleber, Florian and Labahn, Roger and Lang, Eva Maria and Laube, Sören and Leifert, Gundram and Louloudis, Georgios and McNicholl, Rory and Meunier, Jean-Luc and Michael, Johannes and Mühlbauer, Elena and Philipp, Nathanael and Pratikakis, Ioannis and Puigcerver Pérez, Joan and Putz, Hannelore and Retsinas, George and Romero, Verónica and Sablatnig, Robert and Sánchez, Joan Andreu and Schofield, Philip and Sfikas, Giorgos and Sieber, Christian and Stamatopoulos, Nikolaos and Strauß, Tobias and Terbul, Tamara and Toselli, Alejandro Héctor and Ulreich, Berthold and Villegas, Mauricio and Vidal, Enrique and Walcher, Johanna and Weidemann, Max and Wurster, Herbert and Zagoris, Konstantinos},
  date = {2019-09-09},
  journaltitle = {Journal of Documentation},
  shortjournal = {JD},
  volume = {75},
  number = {5},
  pages = {954--976},
  issn = {0022-0418},
  doi = {10.1108/JD-07-2018-0114},
  url = {https://www.emerald.com/insight/content/doi/10.1108/JD-07-2018-0114/full/html},
  urldate = {2022-08-04},
  abstract = {Purpose – An overview of the current use of handwritten text recognition (HTR) on archival manuscript material, as provided by the EU H2020 funded Transkribus platform. It explains HTR, demonstrates Transkribus, gives examples of use cases, highlights the affect HTR may have on scholarship, and evidences this turning point of the advanced use of digitised heritage content. The paper aims to discuss these issues.},
  langid = {english},
  file = {/Users/kellychristensen/Zotero/storage/8UK6P99F/Muehlberger et al. - 2019 - Transforming scholarship in the archives through h.pdf}
}

@inreference{muzerelleCaviarder2011,
  title = {Caviarder},
  booktitle = {Codicologia},
  author = {Muzerelle, Denis},
  date = {2011},
  publisher = {{Institut de recherche et d'histoire des textes}},
  url = {http://codicologia.irht.cnrs.fr/theme/liste_theme/413#tr-868},
  urldate = {2022-07-25}
}

@book{muzerelleVocabulaireCodicologiqueRepertoire1985,
  title = {Vocabulaire Codicologique : Répertoire Méthodique Des Termes Français Relatifs Aux Manuscrits},
  author = {Muzerelle, Dennis},
  date = {1985},
  series = {Rubricae},
  number = {1},
  publisher = {{Éd. Cemi}},
  location = {{Paris}}
}

@software{NumpyNumPyFundamental,
  title = {Numpy: {{NumPy}} Is the Fundamental Package for Array Computing with {{Python}}.},
  shorttitle = {Numpy},
  url = {https://www.numpy.org},
  urldate = {2022-08-04},
  version = {1.23.1},
  keywords = {Scientific/Engineering,Software Development},
  file = {/Users/kellychristensen/Zotero/storage/QF4T3US4/numpy.html}
}

@inproceedings{ortizsuarezAsynchronousPipelineProcessing2019,
  title = {Asynchronous {{Pipeline}} for {{Processing Huge Corpora}} on {{Medium}} to {{Low Resource Infrastructures}}},
  booktitle = {7th {{Workshop}} on the {{Challenges}} in the {{Management}} of {{Large Corpora}} ({{CMLC-7}})},
  author = {Ortiz Suárez, Pedro Javier and Sagot, Benoît and Romary, Laurent},
  editor = {Bański, Piotr and Barbaresi, Adrien and Biber, Hanno and Breiteneder, Evelyn and Clematide, Simon and Kupietz, Marc and Lüngen, Harald and Iliadi, Caroline},
  date = {2019-07},
  publisher = {{Leibniz-Institut für Deutsche Sprache}},
  location = {{Cardiff, United Kingdom}},
  doi = {10.14618/IDS-PUB-9021},
  url = {https://hal.inria.fr/hal-02148693},
  urldate = {2022-09-07},
  abstract = {Common Crawl is a considerably large, heterogeneous multilingual corpus comprised of crawled documents from the internet, surpassing 20TB of data and distributed as a set of more than 50 thousand plain text files where each contains many documents written in a wide variety of languages. Even though each document has a metadata block associated to it, this data lacks any information about the language in which each document is written, making it extremely difficult to use Common Crawl for monolingual applications. We propose a general, highly parallel, multithreaded pipeline to clean and classify Common Crawl by language; we specifically design it so that it runs efficiently on medium to low resource infrastructures where I/O speeds are the main constraint. We develop the pipeline so that it can be easily reapplied to any kind of  heterogeneous corpus and so that it can be parameterised to a wide range of infrastructures. We also distribute a 6.3TB version of Common Crawl, filtered, classified by language, shuffled at line level in order to avoid copyright issues, and ready to be used for NLP applications.},
  file = {/Users/kellychristensen/Zotero/storage/I4S9B6RC/Ortiz Suárez et al. - 2019 - Asynchronous Pipeline for Processing Huge Corpora .pdf}
}

@book{ostosVocabularioCodicologIaVersion1997,
  title = {Vocabulario de {{codicologÍa}} : Versión Española Revisada y Aumentada Del Vocabulaire Codicologique},
  author = {Ostos, Pilar and Pardo, M. Luisa and Rodríguez, Elena E.},
  date = {1997},
  series = {Instrumenta Bibliologica},
  publisher = {{Arco/Libros}},
  location = {{Madrid}}
}

@article{otsuThresholdSelectionMethod1979,
  title = {A {{Threshold Selection Method}} from {{Gray-Level Histograms}}},
  author = {Otsu, Nobuyuki},
  date = {1979-01},
  journaltitle = {IEEE Transactions on Systems, Man, and Cybernetics},
  volume = {9},
  number = {1},
  pages = {62--66},
  abstract = {A nonparametric and unsupervised method of automatic threshold selection for picture segmentation is presented. An optimal threshold is selected by the discriminant criterion, namely, so as to maximize the separability of the resultant classes in gray levels. The procedure is very simple, utilizing only the zeroth- and the first-order cumulative moments of the gray-level histogram. It is t straightforward to extend the method to multithreshold problems.},
  langid = {english},
  file = {/Users/kellychristensen/Zotero/storage/HWBYE3C8/Otsu - A Tlreshold Selection Method from Gray-Level Histo.pdf}
}

@inproceedings{pelletProjetAGODAOcerisation2022,
  title = {Le Projet {{AGODA}}. {{Océrisation}} Des Débats Parlementaires Français de La {{Troisième République}} : Problèmes, Défis et Perspectives},
  author = {Pellet, Aurélien and Puren, Marie},
  date = {2022-04},
  location = {{Le Kremlin-Bicêtre, France}},
  url = {https://hal.archives-ouvertes.fr/hal-03651146},
  urldate = {2022-08-29},
  abstract = {Dans cette intervention, Aurélien Pellet et Marie Puren présenteront le projet AGODA, financé par le DataLab de la Bibliothèque nationale de France, qui a pour objectif de mettre en ligne les comptes-rendus annotés des débats parlementaires de la fin de la Troisième République (1889-1893). Ce projet est conçu comme une preuve de concept, permettant de mettre en place les différents outils nécessaires au traitement de cette importante masse de documents historiques. Les intervenants se concentreront plus particulièrement sur l’océrisation de ces documents : l’extraction du texte, à partir de ces documents numérisés, est en effet une étape décisive, qui n’est pas sans poser beaucoup de problèmes. La qualité des documents et de la numérisation rend en effet difficile cette opération. Aurélien Pellet et Marie Puren présenteront ainsi les problèmes rencontrés et les solutions choisies pour les surmonter.},
  eventtitle = {Séminaire {{OMNSH-Epitech}} : Le Numérique Au Service Des Sciences Humaines et Sociales},
  file = {/Users/kellychristensen/Zotero/storage/8323VRGI/hal-03651146.html}
}

@inproceedings{pincheCremmaLabProjectTranscription2022,
  title = {{{CremmaLab}} Project: {{Transcription}} Guidelines and {{HTR}} Models for {{French}} Medieval Manuscripts},
  shorttitle = {{{CremmaLab}} Project},
  booktitle = {Colloque ”{{Documents}} Anciens et Reconnaissance Automatique Des Écritures Manuscrites”},
  author = {Pinche, Ariane and Camps, Jean-Baptiste},
  date = {2022-06},
  location = {{Paris, France}},
  url = {https://hal.archives-ouvertes.fr/hal-03716526},
  urldate = {2022-08-10},
  file = {/Users/kellychristensen/Zotero/storage/QZUTS62T/hal-03716526.html}
}

@online{pincheEditionLiSeintConfessorJns915Jns1856,
  title = {{{EditionLiSeintConfessor}}/Jns915.Jns1856.Ciham-Fr1.Xml at Master · {{ArianePinche}}/{{EditionLiSeintConfessor}}},
  author = {Pinche, Ariane},
  url = {https://github.com/ArianePinche/EditionLiSeintConfessor/blob/master/XML/traductions/jns915.jns1856.ciham-fr1.xml},
  urldate = {2022-08-26},
  file = {/Users/kellychristensen/Zotero/storage/RLIG6764/jns915.jns1856.ciham-fr1.html}
}

@article{pincheHTRModelCremma2022,
  title = {HTR model Cremma Medieval},
  author = {Pinche, Ariane},
  date = {2022-06-21},
  publisher = {{Zenodo}},
  doi = {10.5281/zenodo.6669508},
  url = {https://zenodo.org/record/6669508},
  urldate = {2022-08-10},
  abstract = {This project was funded by the DIM MAP in the context of the CREMMA project (https://www.dim-map.fr/projets-soutenus/cremma/) The cremma-medieval repository was created in order to make available transcription corpora for training HTR models for medieval manuscripts from the 12th to the 14th century. The CREMMA Medieval dataset has been built with eScriptorium (http://traces6.paris.inria.fr), an interface for HTR ground truth production, and, an HTR and layout segmentation engine. It is composed of ten Old French manuscripts written between the 13th and 14th centuries, mainly scanned in high definition and color except for one manuscript (Vatican) which is a black and white document and BnF fr. 17229, 13496 and 411 that come from microfilm scans. The datasets is mostly made from pre-existing transcribed texts and the samples size can be very different from one source manuscript to the other. The basis of the dataset is composed of the following transcriptions : Bibliothèqe nationale de France, Arsenal 3516, Crowdsourced transcriptions of the collaborative projects of the Standford Library: Bestiaire de Guillaume le Clerc de Normandie (https://fromthepage.com/stanfordlibraries/guillaume-le-clerc-de-normandie-s-bestiary) Bibliothèqe nationale de France, fr.~411,~Vie de saint Lambert~transcribed by A. Pinche (ENC) Bibliothèqe nationale de France, fr.~412,~Li Seint Confessor~de Wauchier de Denain transcribed by A. Pinche (ENC) Bibliothèqe nationale de France, fr.~844,~Manuscrit du Roi, Maritem project(https://anr.fr/Projet-ANR-18-CE27-0016) transcribed by V. Mariotti (projet Maritem) Bibliothèqe nationale de France, fr.~13496,~Vie de saint Jérôme~transcribed by A. Pinche (ENC) Bibliothèqe nationale de France, fr.~17229,~Vie de saint Jérôme~transcribed by A. Pinche (ENC) Bibliothèqe nationale de France, fr.~25516,~Beuve de Hantone~transcribed By A. Nolibois (Université d'Aix-Marseille) Bibliothèqe nationale de France, fr.22550,~Les Sept Sages de Thèbes, this project just started in Geneva under the direction of Y. Foehr-Janssen (UNIGE), the different have been transcribed by Camille Carnaille (ULB/UNIGE) (fol.157r, 163v, 174v, 178v, 186v, 200v), Prunelle Deleville (UNIGE) (fol. 157v, 178r, 186r, 200r, 204r, 343v), Sophie Lecomte (ULB) (fol. 174v), Aminoel Meylan (UNIGE) (169r), Simone Ventura (ULB) (fol. 163r). Cologny, Bodmer, 168 and Vatican, Reg. Lat., 1616,~Chanson d'Otinel~transcribed by J.~-B.~Camps (ENC) from the Geste project (https://github.com/Jean-Baptiste-Camps/Geste) University of pennsylvania, codex 660, pelerinage de mademoiselle Sapience, transcribe by Ariane Pinche (ENC) University of pennsylvania, codex 909,~Énéide, transcribed by Lucien Dugaz (ENC) As the data come from different projects, transcriptions have been standardized to strengthen HTR models. We chose a graphemic transcription method, following D. Stutzmann definitions (see bibliography), to have a sign in the image corresponding to a sign in our text: all the abbreviations are kept, and u/v or i/j are not distinguished. The spaces in the dataset are not homogeneously represented, sometimes transcriptions reproduce the manuscript spacing while others use lexical spaces. It must be stressed that spaces are the most important source of error in medieval HTR models. Most of the transcription follow the layout segmentation of the SegmOnto ontology (https://github.com/SegmOnto/examples), separating the main column, margin, numbering, drop capital, etc. To ensure the quality of the data, continuous integration workflow (Github Actions) has been put in place checking the segmentation vocabulary : SegmentoKraken, XML schema validator (segmentoAltoValidator.xsd), but also the homogeneity of the signs of the characters used in the dataset through a list of authorized signs and translation table (table.csv) with ChocoMufin.},
  langid = {fro},
  keywords = {kraken_pytorch},
  file = {/Users/kellychristensen/Zotero/storage/S8JWSABY/6669508.html}
}

@software{pincheHTRUnitedCremmamedievalCortado2022,
  title = {{{HTR-United}}/Cremma-Medieval: {{Cortado}} 2.0.0},
  shorttitle = {{{HTR-United}}/Cremma-Medieval},
  author = {Pinche, Ariane and Clérice, Thibault},
  date = {2022-07-11},
  doi = {10.5281/zenodo.6818057},
  url = {https://zenodo.org/record/6818057},
  urldate = {2022-08-12},
  abstract = {Model mixing cremma-medieval dataset with early prints (15e s.) from Gallic(orpor)a project. Test score (Based on CER) : 95,54 \%},
  organization = {{Zenodo}},
  file = {/Users/kellychristensen/Zotero/storage/F9NJ4N4K/6818057.html}
}

@inproceedings{regnaultChallengesLanguageChange2019,
  title = {Challenges of Language Change and Variation: Towards an Extended Treebank of {{Medieval French}}},
  shorttitle = {Challenges of Language Change and Variation},
  booktitle = {{{TLT}} 2019 - 18th {{International Workshop}} on {{Treebanks}} and {{Linguistic Theories}}},
  author = {Regnault, Mathilde and Prévost, Sophie and Villemonte de La Clergerie, Éric},
  date = {2019-08},
  location = {{Paris, France}},
  url = {https://hal.inria.fr/hal-02272560},
  urldate = {2022-08-10},
  abstract = {In order to automatically extend a treebank of Old French (9 th-13 th c.) with new texts in Old and Middle French (14 th-15 th c.), we need to adapt tools for syntactic annotation. However, these stages of French are subjected to great variation, and parsing historical texts remains an issue. We chose to adapt a symbolic system, the French Metagrammar (FRMG), and develop a lexicon comparable to the Lefff lexicon for Old and Middle French. The final goal of our project is to model the evolution of language through the whole period of Medieval French (9 th-15 th c.).},
  file = {/Users/kellychristensen/Zotero/storage/4CT4GTK9/Regnault et al. - 2019 - Challenges of language change and variation towar.pdf}
}

@inproceedings{sagotLefffFreelyAvailable2010,
  title = {The {{Lefff}}, a Freely Available and Large-Coverage Morphological and Syntactic Lexicon for {{French}}},
  booktitle = {7th International Conference on {{Language Resources}} and {{Evaluation}} ({{LREC}} 2010)},
  author = {Sagot, Benoît},
  date = {2010-05},
  location = {{Valletta, Malta}},
  url = {https://hal.inria.fr/inria-00521242},
  urldate = {2022-08-10},
  abstract = {In this paper, we introduce the Lefff , a freely available, accurate and large-coverage morphological and syntactic lexicon for French, used in many NLP tools such as large-coverage parsers. We first describe Alexina, the lexical framework in which the Lefff is developed as well as the linguistic notions and formalisms it is based on. Next, we describe the various sources of lexical data we used for building the Lefff , in particular semi-automatic lexical development techniques and conversion and merging of existing resources. Finally, we illustrate the coverage and precision of the resource by comparing it with other resources and by assessing its impact in various NLP tools.},
  file = {/Users/kellychristensen/Zotero/storage/YXDDAEND/Sagot - 2010 - The Lefff, a freely available and large-coverage m.pdf}
}

@inproceedings{scheithauerEScriptoriumTEIPublisher2021,
  title = {From {{eScriptorium}} to {{TEI Publisher}}},
  booktitle = {Brace Your Digital Scholarly Edition!},
  author = {Scheithauer, Hugo and Chagué, Alix and Romary, Laurent},
  date = {2021-11},
  location = {{Berlin, France}},
  url = {https://hal.inria.fr/hal-03538115},
  urldate = {2022-08-23},
  keywords = {Automatic transcription,Handwritten text recognition,HTR,Optical character recognition OCR,TEI,XSLT},
  file = {/Users/kellychristensen/Zotero/storage/GXEV6BCV/Scheithauer et al. - 2021 - From eScriptorium to TEI Publisher.pdf}
}

@article{schneiderProductionMachinereadableText1971,
  title = {The Production of Machine-Readable Text: {{Some}} of the Variables},
  shorttitle = {The Production of Machine-Readable Text},
  author = {Schneider, Ben R.},
  date = {1971-09},
  journaltitle = {Computers and the Humanities},
  shortjournal = {Comput Hum},
  volume = {6},
  number = {1},
  pages = {39--47},
  issn = {0010-4817, 1572-8412},
  doi = {10.1007/BF02402324},
  url = {http://link.springer.com/10.1007/BF02402324},
  urldate = {2022-08-02},
  langid = {english},
  file = {/Users/kellychristensen/Zotero/storage/D33T3DJE/Schneider - 1971 - The production of machine-readable text Some of t.pdf}
}

@article{siddiqiWritingPropertyDescriptors2011,
  title = {Writing Property Descriptors : A Proposal for Typological Groupings},
  shorttitle = {Writing Property Descriptors},
  author = {Siddiqi, Imran-Ahmed and Cloppet, Florence and Vincent, Nicole},
  date = {2011},
  journaltitle = {Gazette du livre médiéval},
  volume = {56},
  number = {1},
  pages = {42--57},
  publisher = {{Persée - Portail des revues scientifiques en SHS}},
  doi = {10.3406/galim.2011.1981},
  url = {https://www.persee.fr/doc/galim_0753-5015_2011_num_56_1_1981},
  urldate = {2022-08-02},
  langid = {english},
  file = {/Users/kellychristensen/Zotero/storage/BKK73FAN/galim_0753-5015_2011_num_56_1_1981.html}
}

@online{SRUSearchRetrieval,
  title = {{{SRU}}: {{Search}}/{{Retrieval}} via {{URL}} -- {{SRU}}, {{CQL}} and {{ZeeRex}} ({{Standards}}, {{Library}} of {{Congress}})},
  url = {https://www.loc.gov/standards/sru/},
  urldate = {2022-09-02},
  file = {/Users/kellychristensen/Zotero/storage/JCH8Z6UU/sru.html}
}

@article{stehnoMETAeAutomatedEncoding2003,
  title = {{{METAe--Automated Encoding}} of {{Digitized Texts}}},
  author = {Stehno, Birgit and Egger, Alexander and Retti, Gregor},
  date = {2003-04-01},
  journaltitle = {Literary and Linguistic Computing},
  shortjournal = {Literary and Linguistic Computing},
  volume = {18},
  number = {1},
  pages = {77--88},
  issn = {0268-1145, 1477-4615},
  doi = {10.1093/llc/18.1.77},
  url = {https://academic.oup.com/dsh/article-lookup/doi/10.1093/llc/18.1.77},
  urldate = {2022-08-24},
  abstract = {This paper explains why and how the digitization project METAe applies METS (Metadata Encoding and Transmission Standard) as encoding scheme for automatically extracted metadata. In contrast to TEI (Text Encoding Initiative) and other markup languages, METS allows encoding of the whole range of structural, descriptive, and administrative metadata in a systematic way. As the METS schema permits the integration of other existing standards, it provides a highly flexible output that can be converted easily to the individual needs of digital libraries. An innovative aspect of the METAe data structure is the ALTO file (‘Analysed layout and text object’), which contains the layout structures as well as the text passages of book pages. Structural maps of the METS schema are used to compose the logical and the physical structures out of ALTO and image files.},
  langid = {english},
  file = {/Users/kellychristensen/Zotero/storage/KAMBCR7H/Stehno - 2003 - METAe--Automated Encoding of Digitized Texts.pdf}
}

@inproceedings{steinOldFrenchDependency2016,
  title = {Old {{French}} Dependency Parsing : {{Results}} of Two Parsers Analysed from a Linguistic Point of View},
  author = {Stein, Achim},
  date = {2016},
  pages = {707--713},
  publisher = {{European Language Resources Association (ELRA)}},
  location = {{Portorož, Slovenia}},
  abstract = {The treatment of medieval texts is a particular challenge for parsers. I compare how two dependency parsers, one graph-based, the other transition-based, perform on Old French, facing some typical problems of medieval texts: graphical variation, relatively free word order, and syntactic variation of several parameters over a diachronic period of about 300 years. Both parsers were trained and evaluated on the Syntactic Reference Corpus of Medieval French (SRCMF), a manually annotated dependency treebank. I discuss the relation between types of parsers and types of language, as well as the differences of the analyses from a linguistic point of view.},
  eventtitle = {Proceedings of the {{Tenth International Conference}} on {{Language Resources}} an  {{Evaluation}} ({{LREC}}’16)},
  langid = {english},
  file = {/Users/kellychristensen/Zotero/storage/D9KPCB8P/Stein - Old French Dependency Parsing Results of Two Pars.pdf}
}

@book{steinSyntacticAnnotationMedieval2013,
  title = {Syntactic Annotation of Medieval Texts: The {{Syntactic Reference Corpus}} of {{Medieval French}} ({{SRCMF}})},
  shorttitle = {Syntactic Annotation of Medieval Texts},
  author = {Stein, Achim and Prévost, Sophie},
  date = {2013},
  pages = {275},
  publisher = {{Narr Verlag}},
  url = {https://halshs.archives-ouvertes.fr/halshs-01122079},
  urldate = {2022-08-10},
  abstract = {This article presents the Syntactic Reference Corpus of Medieval French (SRCMF).},
  isbn = {978-3-8233-6760-4},
  langid = {english},
  file = {/Users/kellychristensen/Zotero/storage/HT4KY8MQ/Stein and Prévost - 2013 - Syntactic annotation of medieval texts the Syntac.pdf;/Users/kellychristensen/Zotero/storage/FDL2LFIZ/halshs-01122079.html}
}

@book{steinSyntacticReferenceCorpus2013,
  title = {Syntactic {{Reference Corpus}} of {{Medieval French}} ({{SRCMF}})},
  author = {Stein, Achim and Prévost, Sophie},
  date = {2013},
  publisher = {{ILR University of Stuttgart}},
  location = {{Stuttgart}},
  url = {http://srcmf.org},
  isbn = {899-492-963-833-3}
}

@online{TeamSegmOntoDocumentation,
  title = {The Team - {{SegmOnto Documentation}}},
  url = {https://segmonto.github.io/pj/team/},
  urldate = {2022-07-25},
  file = {/Users/kellychristensen/Zotero/storage/2RNV576E/team.html}
}

@online{TEIElementFacsimile,
  title = {TEI element facsimile},
  url = {https://tei-c.org/release/doc/tei-p5-doc/en/html/ref-facsimile.html},
  urldate = {2022-09-03},
  langid = {SCHEME=iso639 en},
  file = {/Users/kellychristensen/Zotero/storage/XICXB6WH/ref-facsimile.html}
}

@online{TEIElementSourceDoc,
  title = {TEI element sourceDoc},
  url = {https://tei-c.org/release/doc/tei-p5-doc/en/html/ref-sourceDoc.html},
  urldate = {2022-08-23},
  langid = {SCHEME=iso639 en},
  file = {/Users/kellychristensen/Zotero/storage/J2DWECPZ/ref-sourceDoc.html}
}

@software{TEIPublisherShakespeare2022,
  title = {{{TEI Publisher Shakespeare Demo}}},
  date = {2022-08-08T15:28:49Z},
  origdate = {2020-05-26T12:28:01Z},
  url = {https://github.com/eeditiones/shakespeare/blob/74a9e3269c29e3ccb0b86fe293fb0e90826129be/data/F-3h6.xml},
  urldate = {2022-08-27},
  abstract = {A demo generated from TEI Publisher to browse the Bodleian First Folio of Shakespeare Plays},
  organization = {{e-editiones.org}}
}

@online{TEITextEncoding,
  title = {{{TEI}}: {{Text Encoding Initiative}}},
  url = {https://tei-c.org/},
  urldate = {2022-08-24},
  file = {/Users/kellychristensen/Zotero/storage/9YGXEWWP/tei-c.org.html}
}

@online{TEITextEncodinga,
  title = {About – {{TEI}}: {{Text Encoding Initiative}}},
  url = {https://tei-c.org/about/},
  urldate = {2022-08-25},
  file = {/Users/kellychristensen/Zotero/storage/9FHJXHE3/about.html}
}

@letter{tfibelRETrGallic2022,
  type = {E-mail},
  title = {{{RE}}: {{Tr}} : [{{Gallic}}(Orpor)a] {{Question}} Sur l'{{Unimarc}} Du Catalogue Généra},
  author = {Tfibel, Florence},
  date = {2022-07-13}
}

@article{truanBuildingEncodingAnnotating2021,
  title = {Building, {{Encoding}}, and {{Annotating}} a {{Corpus}} of {{Parliamentary Debates}} in {{TEI XML}}: {{A Cross-Linguistic Account}}},
  shorttitle = {Building, {{Encoding}}, and {{Annotating}} a {{Corpus}} of {{Parliamentary Debates}} in {{TEI XML}}},
  author = {Truan, Naomi and Romary, Laurent},
  date = {2021-03-17},
  journaltitle = {Journal of the Text Encoding Initiative},
  number = {Issue 14},
  publisher = {{Text Encoding Initiative Consortium}},
  issn = {2162-5603},
  doi = {10.4000/jtei.4164},
  url = {https://journals.openedition.org/jtei/4164#tocto2n4},
  urldate = {2022-08-26},
  abstract = {This paper introduces an integrative and comprehensive method for the linguistic                annotation of parliamentary discourse. Initially conceived as documentation for a                specific and small-scale research project, the annotation scheme takes into account                national specificities and is geared to proposing an annotation scheme that is both                highly standardized and adaptable to other research contexts. In this paper we                present a specific application of the Text Encoding Initiative (TEI) framework                applied to a subset of official transcripts of plenary proceedings in three                parliamentary cultures. The TEI annotation scheme proposed here has two main                applications: first, it serves as a basis for encoding parliamentary corpora by                providing a systematic way of annotating both elements within the text (e.g., turns,                incidents, and interruptions) and the metadata associated with it (e.g., variables                pertaining to the speaker or the speech event); second, it provides a                cross-linguistic empirical basis for further annotation projects.},
  issue = {Issue 14},
  langid = {english},
  keywords = {annotation,contrastive linguistics,parliamentary debates},
  file = {/Users/kellychristensen/Zotero/storage/RBK7JT37/Truan and Romary - 2021 - Building, Encoding, and Annotating a Corpus of Par.pdf}
}

@article{turingComputingMachineryIntelligence1950,
  title = {Computing {{Machinery}} and {{Intelligence}}},
  author = {Turing, A. M.},
  date = {1950-10-01},
  journaltitle = {Mind},
  shortjournal = {Mind},
  volume = {LIX},
  number = {236},
  pages = {433--460},
  issn = {0026-4423},
  doi = {10.1093/mind/LIX.236.433},
  url = {https://doi.org/10.1093/mind/LIX.236.433},
  urldate = {2022-08-04},
  file = {/Users/kellychristensen/Zotero/storage/A3JAIIG2/TURING - 1950 - I.—COMPUTING MACHINERY AND INTELLIGENCE.pdf;/Users/kellychristensen/Zotero/storage/ADX28XD4/986238.html}
}

@article{zarriQuelquesAspectsTechniques1977,
  title = {Quelques aspects techniques de l'exploitation informatique des documents textuels : saisie des données et problèmes de sortie},
  shorttitle = {Quelques aspects techniques de l'exploitation informatique des documents textuels},
  author = {Zarri, Gian Piero},
  date = {1977},
  journaltitle = {Publications de l'École Française de Rome},
  volume = {31},
  number = {1},
  pages = {399--413},
  publisher = {{Persée - Portail des revues scientifiques en SHS}},
  url = {https://www.persee.fr/doc/efr_0000-0000_1977_act_31_1_2286},
  urldate = {2022-08-01},
  langid = {fre},
  file = {/Users/kellychristensen/Zotero/storage/9PP8TLPN/Zarri - 1977 - Quelques aspects techniques de l'exploitation info.pdf;/Users/kellychristensen/Zotero/storage/67VGIS5R/efr_0000-0000_1977_act_31_1_2286.html}
}


