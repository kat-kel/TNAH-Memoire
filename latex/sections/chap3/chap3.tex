% PREAMBULE
% !BIB TS-program = biber
% !TEX TS-program = xelatexmk
% ITEX TS-program = latex
% !TEX spellcheck = French

\documentclass[class=article, crop=false]{standalone}
\usepackage[subpreambles=true]{standalone}
\usepackage{import}
\usepackage{blindtext}

%%%%%%%%%%%%%%%%%%%%%%%%
%			REFERENCES
% le package hyperref avec des options, si en local
\usepackage{hyperref}
\usepackage[backend=biber, sorting=nyt, style=verbose-ibid]{biblatex}
\addbibresource{../../../bib.bib}

%%%%%%%%%%%%%%%%%%%%%%%%
%			GLOSSAIRE
\usepackage[acronym]{glossaries}
\makeglossaries
\newglossaryentry{htr}
{
    name=Handwritten Text Recognition,
    description={La reconnaissance du texte écrit sur une image numérique}
}
\newacronym{HTR}{HTR}{Handwritten Text Recognition}

\newglossaryentry{ocr}
{
    name=Optical Character Recognition,
    description={La reconnaissance des polices du texte sur une image numérique}
}
\newacronym{OCR}{OCR}{Optical Character Recognition}

\newglossaryentry{Inria}
{
    name=Inria,
    description={Institut national de recherche en sciences et technologies du numérique}
}
\newacronym{INRIA}{Inria}{Institut national de recherche en sciences et technologies du numérique}

\newglossaryentry{enc}
{
    name=École nationale des chartes,
    description={Grande école bla bla bla}
}
\newacronym{ENC}{ENC}{École nationale des chartes}

\newglossaryentry{HTR-United}
{
    name=HTR-United,
    description={HTR-United is a catalog and an ecosystem for sharing and finding ground truth for optical character or handwritten text recognition (OCR/HTR)}
}

\newglossaryentry{CLab}
{
	name=CREMMALab,
	description={Consortium pour la reconnaissance
d’'écriture manuscrite des matériaux anciens}
}
\newacronym{CREMMA}{CREMMA}{Consortium Reconnaissance
d’Écriture Manuscrite des Matériaux Anciens}

\newglossaryentry{TEI}
{
	name={Text Encoding Initiative},
	description={Normes internationales de l'encodage des documents texts}
}
\newacronym{tei}{TEI}{Text Encoding Initiative}

\newglossaryentry{iiif}
{
	name={International Image Interoperability Framework},
	description={Normes internationales de l'exploitation des images numériques et de leurs métadonnées par API}
}
\newacronym{IIIF}{IIIF}{International Image Interoperability Framework}

\newacronym{alto}{ALTO}{Analyzed Layout and Text Object}
\newacronym{xml}{XML}{eXtensible Markup Language}
\newacronym{BNF}{BnF}{Bibliothèque nationale de France}
\newacronym{almanach}{ALMAnaCH}{Automatic Language Modelling and Analysis \& Computational Humanities}
\newacronym{rdf}{RDF}{Resource Description Framework}
\newacronym{TAL}{TAL}{Traitement automatique des langues}


\begin{document}
Le projet \textit{Gallic(orpor)a} s'est développé à partir de plusieurs projets précédents et il tire parti de divers domaines de recherche. Ses créateurs, en mettant en valeur leurs propres connaissances, ont visé à assembler un pipeline qui peut traiter tout document dans la base de données Gallica de la \acrlong{BNF} (\acrshort{BNF}). Les chercheurs spécialisés en \textit{l'\acrlong{HTR}} (\acrshort{HTR}), en le \acrlong{TAL} (\acrshort{TAL}), en l'histoire, en la littérature, en la lexicographie et en la stylométrie se sont rassemblés pour réaliser ce pipeline. Le pipeline visait à prédire et analyser du ancien français et du français de l'Ancien Régime, ainsi que les manuscrits et les imprimés, à partir des pages numérisées des documents créés entre 1400 et la révolution française. Cependant, le vrai rêve du projet était de produire un prototype qui servirait d'exemple et pourrait être élaboré dans le but de traiter vraiment tout document source numérisé.

Les ambitions du \textit{Gallic(orpor)a} se sont rendu possibles grâce aux recherches de plusieurs chercheurs et ingénieurs, tel que Laurent Romary, Philippe Gambette, Thibault Clérice, Pedro Suarez Ortiz, Claire Jahan, Caroline Corbières, et Alexandre Bartz. Mais les principaux qui se chargeaient de la surveillance du projet \textit{Gallic(orpor)a} lors de mon stage en 2022 étaient Jean-Baptiste Camps, Simon Gabay, et Ariane Pinche, qui ont développé des modèles \acrshort{HTR} pour extraire du texte des document numériques dans la base de données Gallica. Chez \Gls{Inria}, en tant que stagiaire, j'ai aussi travaillé en collaboration avec  Benoît Sagot et Rachel Bawden, qui ont développé des outils d'analyse linguistique du texte extrait. Tous ensemble, ces chercheurs de divers spécialités ont contribué leurs connaissances pour produire un processus du traitement polyvalent.

%%%%%%%%%%%%%%%%%%%%%%%%
%			1. CONTEXTE DU PROJET
\section{Le contexte du projet}

\subsubsection{Bibliothèque nationale de France et le Data Lab}
Le Data Lab s'est mis en place au sein de la \acrlong{BNF} (\acrshort{BNF}) en 2021.\footcite{carlinBnFDataLabService2021} Lors de sa première année, le Data Lab a lancé son premier appel aux projets qui mettent en valeur les fonds et les ressources de l'institution phare patrimoniale. Le projet \textit{Gallic(orpor)a} faisait partie des premiers projets acceptés en 2021, à côté des projets \textit{AUREJ} (Accès Unifié aux REssources de la Jouabilité), \textit{GALLICAENV}, \textit{BUZZ-F}, et \textit{AGODA} (Analyse sémantique et Graphes relationnels pour l’Ouverture et l’étude des Débats à l’Assemblée nationale).\footcite[123]{bibliothequenationaledefranceRapportActivite20212022} Ayant sa candidature retenu, \textit{Gallic(orpora)} profitait du financement du Data Lab de la \acrshort{BNF}. La plupart du travail sur le projet a eu lieu pendant la première moitié de 2022, suite au mis en place du stage et des vacations par Ariane Pinche, Simon Gabay, et Benoît Sagot.

\subsubsection{Inria et l'équipe ALMAnaCH}
\Gls{Inria} est l'\acrlong{INRIA} et il compte plusieurs branches dans le monde. La branche parisienne encadre l'équipe \acrshort{almanach} dont le acronyme veut dire \textit{\acrlong{almanach}}. Au sein d'\acrshort{almanach} s'est développé le meilleur modèle \acrshort{TAL} pour la langue française, CamemBERT.\footcite{martinCamemBERTTastyFrench2020} L'équipe \acrshort{almanach} encadre les chercheurs, les ingénieurs, les doctorants, et les stagiaires attachés aux projets concernés soit par le traitement automatique des langues, soit par les humanités numériques. L'acronyme du nom fait référence à ces deux pôles de recherche~: \textit{Automatic Language Modelling and Analysis} est le traitement automatique des langues, et le \textit{Computational Humanities} est l'humanités numériques. Le projet \textit{Gallic(orpor)a} se situait entre les deux, impliquant l'extraction des données et l'édition des documents historiques ainsi que l'analyse linguistique du texte extrait. 

Le directeur de recherches d'\acrshort{almanach} est Benoît Sagot, qui s'est chargé de l'encadrement du stage du projet \textit{Gallic(orpor)a}. En tant que stagiaire, je faisais partie de l'équipe entre début avril et fin juillet 2022. Pendant le stage, Rachel Bawden a animé un groupe de lecture hebdomadaire et des séminaires mensuelles dont j'ai profité dans l'intérêt de me tenir au courant sur les nouvelles recherches et les nouveaux enjeux du \acrshort{TAL}. Elle a aussi développé un modèle \acrshort{TAL} pour le projet \textit{Gallic(orpor)a} et m'a instruit dans sa mise en œuvre.\footcite{bawdenAutomaticNormalisationEarly2022, gabayFreEMcorporaFreEMnormFreEM2022} Disposée d'un bureau, j'ai travaillé en présentiel quatre jours par semaine, passant un jour toutes les semaines à l'\Gls{enc} pour travailler à côté de l'une des chefs du projet, Ariane Pinche. Chez \Gls{Inria}, j'ai profité de l'expertise de mes collègues de l'équipe \acrshort{almanach}, en particulier Alix Chagué et Hugo Scheithauer. L'équipe entière de \textit{Gallic(orpor)a} a aussi profité des serveurs d'\Gls{Inria}, qui prennaient en charge une partie de la puissance de calcul et du stockage de données pour l'interface graphique \acrshort{HTR} \textit{eScriptorium}.

\subsubsection{École nationale des chartes et l'université de Genève}
En tant qu'une école, contrairement à une équipe de recherche comme \acrshort{almanach}, les rôles de l'\Gls{enc} et l'université de Genève dans le projet \textit{Gallic(orpor)a} concernés l'encadrement des chercheurs qui y ont contribué leurs connaissances. L'\Gls{enc} (\acrshort{ENC}) a aussi donné un lieu de travail, dont j'ai profité un jour par semaine. Ariane Pinche, qui était post-doctorante à l'\acrlong{ENC}, et Simon Gabay, maître-assistant à l'université de Genève, ils ont géré la mise en place du stage et des vacations que la bourse du Data Lab de la \acrshort{BNF} a financés pour 2022. L'\acrshort{ENC} et l'université de Genève les ont soutenu lors de l'encadrement des vacations et du stage.

Gabay, Pinche, et deux autres chercheurs qui étaient attachés à l'\acrlong{ENC} pendant le stage, Jean-Baptiste Camps et Thibault Clérice, ont tous contribué au projet \textit{Gallic(orpor)a}. Gabay et Pinche se sont occupés de l'harmonisation des vérités de terrain produites par l'équipe en reliant toute transcription que les vacataires ont faite dans l'interface graphique \textit{eScriptorium}. Pinche et Clérice ont commencé à utilisé les vérités de terrain des documents médiévaux en entraînant des nouveaux modèles de l'\acrshort{HTR} et de la segmentation.\footcite{clericeYALTAiSegmontoManuscript2022} Par rapport à la segmentation, Jean-Baptiste Camps, Pinche, et Gabay ont développé le syntaxe \textit{SegmOnto} qui servait à harmoniser les vérités de terrain produites pour tout document dans le corpus d'entraînement, y compris les manuscrits et les imprimés.\footcite{gabaySegmOntoCommonVocabulary2021} Bien que chaque chercheur ait ses spécialités, ils ont tous collaboré et la division des aspects du projet n'étaient pas aussi fermes qu'ils n'ont pas profité des idées de l'un et de l'autre.

\section{La portée des données d'entraînement}
\label{veritesDeTerrain}
Le projet \textit{Gallic(orpor)a} visait à développer et mettre en pratique les modèles \acrshort{HTR} pouvant traiter tout document français dans la base de données Gallica crée entre le XIVe siècle et la révolution française. Un corpus des documents sources numérisés ainsi qu'une sélection de leurs pages à transcrire étaient choisis comme les données d'entraînement d'un tel modèle \acrshort{HTR}. Les membres de ce corpus sont détaillés dans le section \ref{data:portee} de l'appendice. Le choix se faisait en pensant à la diversité linguistique et à la diversité du genre. Comme montre la Figure \ref{fig:diversite_genre}, il y avait les manuscrits, les incunables, et les imprimés. De plus, chaque type de document a porté plusieurs genres littéraires, y compris la poésie, le récit, et le traité. Dans le cas des imprimés, il y avait aussi des pièces de théâtre.

\begin{figure}[hbt!]
\includegraphics[width=\textwidth]{../../../images/diversite_genre.png}
\caption{Diversité linguistique et générique}
\label{fig:diversite_genre}
\end{figure} 

Pour terminer, un autre souhait quant à la diversité du corpus a porté sur le lieu de publication ou d'apparition du document, comme montre la Figure \ref{fig:diversite_ville}. La plupart des documents choisis de la base de données Gallica étaient sortis de Paris. Une autre partie importante est venue de Lyon. Il y avait aussi un effort d'inclure les manuscrits, les incunables, et les imprimés écrits en français qui sont venus des villes hors de la France, tel que Londres, Amsterdam, et Rome.

\begin{figure}[hbt!]
\includegraphics[width=\textwidth]{../../../images/diversite_ville.png}
\caption{Diversité géographique}
\label{fig:diversite_ville}
\end{figure}

Après avoir établi le corpus, les pages ciblées dans chaque document étaient transcrites par des vacataires en utilisant l'interface graphique \textit{eScriptorium}, qui permet à la fois la transcription du texte et la segmentation de la page. La segmentation de la page se faisant en mettant les étiquettes précises sur les masques des lignes et les bloques de texte~; ces étiquettes suivaient le vocabulaire \textit{SegmOnto}. Ensuite, les vérités de terrain produites avec l'interface graphique \textit{eScriptorium} étaient transférées vers les dépôts Github du bon siècle. L'outil \texttt{HTRVX} du projet \Gls{HTR-United} a analysé toute transcription transféré. L'outil s'est alerté aux erreurs potentielles de la segmentation et il a facilité le nettoyage des données.


\section{Les prédécesseurs du projet}
Comme expliqué avant, \textit{Gallic(orpor)a} a rassemblé les recherches de plusieurs projets précédents. Il a profité des glossaires codicologiques, des progrès dans la prédiction et  la segmentation des documents, des progrès dans l'analyse linguistique, et des catalogues des données. Le glossaire \textit{SegmOnto} se servait à harmoniser les vérités de terrain pour les manuscrits et les imprimés transcrits. Les modèles \acrshort{HTR} étaient à la fois un support à la production des vérités de terrain, en faisant en premier temps une transcription préliminaire, et le but du projet, étant de produire les modèles entraînés sur le vocabulaire \textit{SegmOnto}. Le catalogue \Gls{HTR-United}, spécifiquement son outil \texttt{HTRVX}, a surveillé l'harmonisation des transcriptions produites comme des vérités de terrain.\footcite{clericeHTRVXHTRValidation2021} En outre, le catalogue \Gls{HTR-United} a publié gratuitement les vérités de terrain du projet \textit{Gallic(orpor)a} pour que d'autres projets et d'autres modèles \acrshort{HTR} puissent en profiter.\footcite{chagueSharingHTRDatasets2022} Pour terminer, l'analyse linguistique était aussi un objectif du projet \textit{Gallic(orpor)a}, et la mise en œuvre de cet aspect comptait sur les modèles de la langue française construits par les chercheurs de l'équipe \acrshort{almanach}.


\subsubsection{Le glossaire codicologique}
Le projet \textit{Gallic(orpro)a} a harmonisé ses données selon le vocabulaire \textit{SegmOnto}, qui est expliqué en détail dans le chapitre \ref{chap:segmonto}. Ayant géré les données produites selon cette codicologie, j'ai aussi contribué à l'élaboration et le perfectionnement du vocabulaire. La décision d'utiliser le syntaxe descriptif des lignes et des zones de \textit{SegmOnto} a été prise bien en avance de la mise en œuvre du projet \textit{Gallic(orpor)a}. La généralité du vocabulaire était déterminée d'être conforme à la diversité ciblée des documents traités dans le cadre du projet. Puisque \textit{Gallic(orpor)a} visait à livrer un prototype d'un pipeline qui pourrait traiter tout document source numérisé, la généralisation des étiquettes appliquées aux lignes et aux zones était impérative, et le vocabulaire de \textit{SegmOnto} était jugé la meilleure solution.

\subsubsection{La segmentation et la prédiction du texte}
Les progrès de la reconnaissance du texte sont expliqués en détail dans le chapitre \ref{chap:htr}. Un logiciel \acrshort{HTR} commence par la segmentation de la page, et dès qu'il sait où se trouvent les caractères, les mots, et les lignes du texte il le prédit à partir de l'écriture. Ces deux tâches se font selon les compétences qu'il a appris lors de son entraînement. Dans le but de produire les vérités de terrain pouvant entraîner les modèles \acrshort{HTR} pour les manuscrits, les incunables, et les imprimés dans la base de données Gallica, le projet \textit{Gallic(orpor)a} a profité de l'expertise de Simon Gabay et d'Ariane Pinche, qui s'occupaient de la relecture des vérités de terrain et la gestion des corpus d'or.

 Les progrès dans la prédiction du texte sur les imprimés de l'Ancien Régime ainsi que son analyse ont aidé le projet \textit{Gallic(orpor)a}. Par rapport aux progrès dans l'\acrshort{OCR} des imprimés françaises de l'Ancien Régime, Gabay a entraîné les modèles sur les vérités de terrain des imprimés du XVIe au XVIIIe siècle.\footcite{gabayStandardizingLinguisticData2020} En collaboration avec d'autres chercheurs, il a travaillé sur le jeu de données OCR17+ qui a fourni des vérités de terrain des imprimés du XVIIe siècle.\footcite{gabayOCR17GroundTruth2020} Pour tester le pipeline, dans l'attente des modèles nouvellement entraînés sur les données produites dans le cadre de \textit{Gallic(orpor)a}, j'ai utilisé un modèle de segmentation et un modèle d'\acrshort{HTR} que Gabay a développé dans le cadre de son projet \textit{E-ditiones}.\footcite{gabayEditiones17thFrench2018}

Pour la reconnaissance du texte sur les documents médiévaux, le projet \Gls{CLab} est clef. Géré dans le cadre des études postdoctorales d'Ariane Pinche, le \acrlong{CREMMA}, ou \acrshort{CREMMA}, est un dépôt des images et leurs transcriptions corrigées à la main, c'est-à-dire des vérités de terrain. Afin d'entraîner un modèle \acrshort{HTR}, il faut un jeu des vérités de terrain.\footcite{chagueHTRUnitedMutualisonsVerite2021} Le projet \Gls{CLab} fournit un jeu des vérités de terrain de 13 manuscrits médiévaux qui se composent de 21~656 lignes de texte transcrites.\footcite{pincheCremmaLabProjectTranscription2022} Sur les données du projet, Pinche a entraîné un modèle \acrshort{HTR} qui est désormais disponible sur Zenodo.\footcite{pincheHTRModelCremma2022} Le projet \textit{Gallic(orpor)a} en a profité pour aider à la création des vérités de terrain pour les manuscrits médiévaux de son propre jeu de données.

\subsubsection{L'harmonisation et la partage des données}
Le projet \Gls{HTR-United} mis en commun les vérités de terrain générées par tout projet \textit{open source}. \footcite{chagueHTRUnitedHtrunitedV02022} Sa base de données, gratuitement mise en ligne par GitHub, contient les images et leurs transcriptions faites par plusieurs projets de recherche, et elle porte sur les documents de plusieurs périodes historiques et écritures. Un modèle \acrshort{HTR} peut être entraîné sur ces jeux de données. Par exemple, Alix Chagué a entraîné un modèle \acrshort{HTR} sur les vérités de terrain du \textit{LECTAUREP Project}, soutenu par \Gls{Inria} et les Archives Nationales, qui sont mis en commun sur la base de données \Gls{HTR-United}.\footcite{chagueLECTAUREPContemporaryFrench2022}

Dans l'esprit de la science ouverte, le projet \textit{Gallic(orpor)a} a transféré toute vérité de terrain de ses dépôts GitHub vers le catalogue \Gls{HTR-United}. À la fin du stage, en juillet 2022, Ariane Pinche et Thibault Clérice ont entraîné un modèle pour les manuscrits médiévaux en utilisant les transcriptions que l'équipe du projet \textit{Gallic(orpor)a} ont produites. Ces vérités de terrain sont désormais mises en commun sur \Gls{HTR-United} et Pinche et Clérice ont lié le premier modèle publié du projet \textit{Gallic(orpor)a} avec le catalogue \Gls{HTR-United} et le dépot du projet \acrshort{CREMMA} (\acrlong{CREMMA}).\footcite{pincheHTRUnitedCremmamedievalCortado2022} La partage des données du projet est l'un de ses objectifs.

Ainsi qu'à contribuer au catalogue, le projet \textit{Gallic(orpor)a} a aussi profité des outils de \Gls{HTR-United}. Le dernier met en commun des outils qui ont pour but d'harmoniser les données ajoutés à son catalogue. Ces outils peuvent être intégrés dans un \textit{workflow} de GitHub, ce que l'équipe de \textit{Gallic(orpor)a} a fait. L'un de ces outils est \texttt{HTRVX}, qui se prononce comme le personnage Asterix, et il a rendu possible à l'équipe du projet nettoyer les transcriptions sorties de \textit{eScriptorium}.\footcite{clericeHTRVXHTRValidation2021} En exemple, \texttt{HTRVX} relit les transcriptions et les cherche pour les erreurs communes. L'existence d'un tel outil et sa disponibilité gratuite grâce au projet \Gls{HTR-United} a beaucoup aidé le projet \textit{Gallic(orpor)a}.

\subsubsection{L'analyse linguistique}
Après l'extraction et le nettoyage des données des documents source de Gallica, le projet \textit{Gallic(orpor)a} a envisagé à analyser le texte. Dans cet objectif, il a profité des progrès dans l'analyse linguistique des anciens états de la langue française. L'analyse linguistique du français de l'Ancien Régime, tel que ce qui se voit dans les écrits de Molière, est un domaine de recherche actuellement en plein développement. Depuis une dizaine d'années, les chercheurs dans la linguistique computationnelle ont élaboré des outils pour analyser le français autre que le français contemporaine, dont les recherches sont déjà animées par l'application commerciale et les jeux de données plus nombreuses.

L'histoire de l'analyse linguistique et du \acrlong{TAL} (\acrshort{TAL}) est hors de ce mémoire. Néanmoins, les projets qui ont précédés \textit{Gallic(orpor)a} et sur lesquels il a compté méritent de discussion. Achim Stein a bordé le sujet de l'analyse linguistique du français du Moyen Âge dans son article de 2013.\footcite{steinSyntacticAnnotationMedieval2013} Stein a montré que, pour les chercheurs qui ont débuté d'appliquer les progrès dans l'analyse linguistique à l'étude du français des anciens états, il faut faire attention aux propriétés syntaxiques et morphologiques propres à la langue. Du coup, une architecture qui pourrait parvenir aux résultats souhaités pour l'anglais du Moyen Âge n'aura pas forcement le même taux de réussite avec le français du Moyen Âge à cause de différences syntactiques et morphologiques dans la langue.

Achim Stein et Sophie Prévost ont créé un \textit{treebank} pour l'ancien français, le \textit{Syntactic Reference Corpus of Medieval French} (SRCMF), qui avance toujours l'analyse linguistique des anciens états du français.\footcite{steinSyntacticReferenceCorpus2013} En 2014, Prévost est des autres chercheurs, Gael Guibon, Isabelle Tellier, Matthieu Constant, et Kim Gerdes, ont ajouté aux conclusions de Stein que la variation lexique de l'ancien français pose aussi un défi à l'analyse linguistique.\footcite{guibonParsingPoorlyStandardized2014} En 2019, Mathilde Regnault, Prévost, et Éric Villemonte de La Clergerie ont utilisé le SRCMF avec le MCVF (Modéliser le changement : les voies du français) pour encore élaborer notre connaissance de l'évolution du français.\footcite{regnaultChallengesLanguageChange2019} Telles études linguistiques ont tourné la terre pour que l'analyse du texte extrait des manuscrits médiévaux et des imprimés historiques puissent voir le jour aujourd'hui.

\textit{Gallic(orpor)a} a profité des progrès dans l'analyse linguistique du français historique utilisé dans les imprimés et les manuscrits de ses corpus. Jean-Baptiste Camps, Simon Gabay, Paul Fièvre, Thibault Clérice, et Florian Cafiero ont créé \textit{Deucalion} qui est un modèle \acrshort{TAL} pour le français de l'Ancien Régime, entraîné sur les livrets des drames du XVIIe siècle.\footcite{campsCorpusModelsLemmatisation2021} Encore plus récent est le projet \textit{F{\footnotesize RE}EM}qui veut dire \textit{French Early Modern}\footcite{gabayProjetFREEMRessources2022} Dans une présentation du projet à la conférence du Traitement Automatique des Langues Naturelles à Avignon en juin 2022,  les auteurs ont décrit l'un des modèles qu'ils ont développé à partir de l'étude linguistique de l'ancien français.
\begin{displayquote}
L’étude de la langue ne nécessitant pas uniquement un seul outil, mais toute une gamme de solutions, nous avons adopté une approche holistique du problème, en misant sur la création d’un modèle de langue dédié au français d’Ancien Régime, D’AlemBERT\footcite{gabayFreEMAlemBERT2022}, qui devrait venir en soutien des différentes tâches de TAL envisagées.\footcite{gabayProjetFREEMRessources2022} 
\end{displayquote}
Le développement des nouveaux modèles TAL pour les anciens états du français a rendu possible la mise en œuvre d'un tel étape d'analyse au pipeline de \textit{Gallic(orpro)a}.


%%%%%%%%%%%%%%%%%%%%%%%%
%			3. PIPELINE
\section{Le pipeline}
Ainsi que la création des vérités de terrain, qui est expliqué dans la section précédente de ce chapitre (Section \ref{veritesDeTerrain}), le projet \textit{Gallic(orpor)a} avait pour but de créer un pipeline pouvant traiter tout document source de la base de données Gallica. En mettant en œuvre les modèles entraînés, il visait à générer une ressource lexicographique qui porte sur le document source. En commençant par les pages numérisés du document source, la ressource numérique présentera quatre types d'information~:
\begin{enumerate}
\item les métadonnées à propos du document source
\item les données topologiques et linguistiques prédites par \textit{l'\acrlong{HTR}} (\acrshort{HTR})
\item le texte pré-éditorialisé, extrait du document
\item le texte analysé par les outils \acrlong{TAL} (\acrshort{TAL})
\end{enumerate}
Chaque type d'information veut servir une utilité différente que la lectrice éventuelle ou le lecteur éventuel de la ressource pourrait désirer. Les métadonnées s'informent sur le document source, sa conservation, et sa ressource lexicographique que le pipeline génère. Les données topologiques et linguistiques représentent toute donnée extraite par le modèle \acrshort{HTR}. Le texte pré-éditorialisé et le texte analysé viennent tous les deux des donnés extraites par le modèle. Le texte pré-éditorialisé représente le texte tel qu'il se trouve dans la transcription du document. Et le texte analysé présente le texte extrait après être lemmatisé, normalisé, et analysé par les modèles \acrshort{TAL}.
La saisie des métadonnées et la saisie des données des modèles \acrshort{HTR} et \acrshort{TAL}, tel que le texte extrait et le texte annoté, se fusionnent dans la structure de données XML et selon la norme d'encodage le \textit{\Gls{TEI}} (\acrshort{tei}). Ensuite, ce format sert à reposer sur d'autres structures des données et donc d'autres exploitations, tel que \textit{l'\Gls{iiif}} (\acrshort{IIIF}). Une visualisation du pipeline se voit dans le Figure~\ref{fig:pipeline}.

\begin{figure}[hbt!]
\includegraphics[width=\textwidth]{../../../images/pipeline.png}
\caption{Pipeline}
\label{fig:pipeline}
\end{figure}

\subsection{Les formats déployés dans le pipeline}
Tous les formats de données qui font partie du pipeline se divisent en deux catégories. En premier temps, le pipeline a besoin des images numériques qui composent le fac-similé numérique d'un document source. Ces images peuvent être de plusieurs formats, mais les modèles \acrshort{HTR} que l'utilisatrice ou l'utilisateur a sélectionné pour appliquer doivent pouvoir les traiter. Ensuite, il y a les formats utilisés pour la création des données, qui dans la Figure~\ref{fig:pipeline} s'encadrent dans les carrés gris en pointillé. Le premier, l'extrait du texte (\textit{Text extraction} dans la Figure), se constitue d'un format \acrshort{xml} qui s'appelle \acrshort{alto} (\textit{\acrlong{alto}}) que renvoie le logiciel \acrshort{HTR}. Les modèles \acrshort{TAL} renvoie les données de l'annotation du texte (\textit{Text annotation} dans la Figure) dans une autre structure de données, soit TSV (\textit{tab separated values}) soit CSV (\textit{comma separated values}).

En deuxième temps, il y a les formats déployés dans le pipeline pour exploiter les données ainsi créées~; la Figure~\ref{fig:pipeline} les représente soit en orange soit en violet. Le premier, le \acrshort{tei}, est le format \textit{pivot} qui repose sur les trois autres formats. La ressource lexicographique que le pipeline créé peut donc être exploitée en tant que document \acrshort{tei}, représenté dans la figure en orange. Sinon, le format \textit{pivot} peut être manipulé pour extraire ses métadonnées~; ces dernières peuvent être exploitées dans le format \acrshort{IIIF}, le format DTS (\textit{Distributed Text Services})\footcite{almasDistributedTextServices2021}, ou le format RDF (\textit{Resource Description Framework}), dont la structure des données est un triplet RDF.

\subsection{Les données récupérées}

\subsubsection{Les données topologiques et linguistiques prédites}
À la saisie d'un fac-similé numérique, le pipeline démarre sa phase d'extraction du texte, mise en pointillé dans la Figure~\ref{fig:pipeline}, et déploie ses modèles de segmentation et de l'\acrshort{HTR}. Il peut déployer peu importe quel modèle \acrshort{HTR} qu'une utilisatrice ou un utilisateur veut appliquer au fac-similé numérique, mais le modèle doit produire les données qui conforment au schéma \acrshort{alto}. En séquence, le pipeline applique les modèles à chaque image numérique afin de produire les transcriptions en XML-\acrshort{alto}.

L'un des objectifs du pipeline est de conserver toute donnée produite par les modèles \acrshort{HTR} afin que la ressource lexicographique en XML-\acrshort{tei} qu'il produit puisse se servir à l'entraînement des nouveaux modèles. Les coordonnées des zones et le texte prédit, étant liés au fac-similé numérique, font une vérité de terrain. Ces données topologiques et linguistiques sont récupérées et conservées dans le document \textit{pivot}, la ressource XML-\acrshort{tei}. Ainsi quelqu'un peut extraire les vérités de terrain et les utiliser pour entraîner son propre modèle. La \textit{\acrlong{tei}} fournit un schéma qui convient bien à la conservation de telles données, l'élément XML \texttt{<sourceDoc>}.

\subsubsection{Les métadonnées}
La deuxième saisie de données que prend le pipeline, après le fac-similé numérique, est le nom du dossier dans lequel sont stockées les images du fac-similé. Le pipeline a besoin d'une structure de fichier rigide qu'il construit lors de la récupération des images depuis l'API de la \acrshort{BNF}. Sinon, l'utilisatrice ou l'utilisateur doit l'imiter en appuyant sur ses propres images numériques d'un document source. Mais les images doivent être disponible en ligne par l'API de \textit{l'\Gls{iiif}} (\acrshort{IIIF}) pour que les métadonnées de la source de l'image soient encodées et accessibles par la requête.

\begin{center}
\begin{verbatim}
data/
|____ARK1/
|        |____image1.jpg
|        |____image2.jpg
|        |____image3.jpg
...
|____ARK2/
|        |____image1.jpg
|        |____image2.jpg
|        |____image3.jpg
...
\end{verbatim}
\end{center}

\noindent Toute image numérique doit se trouve dans un dossier qui porte comme nom l'identifiant ARK (\textit{Archival Resource Key}) du document source. Cette clef est importante pour la diffusion et le requêtage des images numériques par les APIs \acrshort{IIIF}. Le pipeline a aussi besoin de cette clef ainsi que de l'association évidente entre elle et les images qui appartiennent au document, qui se donne grâce à la structure de fichier.

Avec cet identifiant ARK, donné par la structure de fichier, le pipeline récupère les métadonnées du document source. Il recherche les informations sur le document source en passant une requête à l'API de l'image \acrshort{IIIF}~; cet API est maintenu par l'institution patrimonial qui se charge de la conservation et/ou de la diffusion en ligne du document. Les métadonnées récupérés de l'API IIIF, géré par l'institution hôte du document numérique, sont liées---si possible---avec des autres sources de données. Dans l'exemple des documents numériques de la base de données Gallica, le pipeline récupère les métadonnées quant au document source depuis l'API \acrshort{IIIF} que la \acrshort{BNF} met à disposition. Si cette requête a réussi, le pipeline va récupérer les données du catalogue général de la \acrshort{BNF} en passant une requête à l'API \textit{SRU} de la \acrshort{BNF} qui veut dire, en anglais, le \textit{Search/Retrieve via URL}. Pour terminer, le pipeline recherche encore des métadonnées dans le catalogue du Système Universitaire de Documentation (SUDOC).

La \textit{\acrlong{tei}} dispose d'un élément XML qui s'appelle le \texttt{<teiHeader>} pour encoder les métadonnées. Le pipeline génère ou récupère et nettoye les métadonnées sur (1) la ressource lexicographique qu'il créé, (2) le fac-similé numérique que les modèles \acrshort{HTR} traite, et (3) le document source dont les pages ont été numérisées. Ces métadonnées sont présentées dans les éléments \acrshort{tei} pertinents qui descende directement ou indirectement de l'élément XML \texttt{<teiHeader>}.

\subsection{Les données produites}
Après avoir créé les métadonnées du document source ainsi que les données topologiques et linguistiques produites par les modèles de segmentation et de l'\acrshort{HTR}, le pipeline continue à traiter les données de la transcription.

\subsubsection{Le texte pré-éditorialisé}
Le pipeline crée 

\subsubsection{Le texte analysé}

\end{document}
\documentclass[../main.tex]{subfiles}